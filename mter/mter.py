import os
import math
import random
import pickle
import argparse
import numpy as np
from util_mter import test
import multi_tasks_paraserver as tsmtr
import sys
from sklearn.preprocessing import normalize
from tqdm import tqdm
sys.path.append("../../data")
from text_processed_dataset import TextProcessedDataset
from yelp_processed_dataset import YelpProcessedDataset
os.environ['CUDA_VISIBLE_DEVICES'] = '2'

def args_process():
    parser = argparse.ArgumentParser()
    parser.add_argument('-u', '--useremb', default=15, type=int, help='user embedding dimension')
    parser.add_argument('-i', '--itememb', default=15, type=int, help='item embedding dimension')
    parser.add_argument('-f', '--featemb', default=12, type=int, help='feature embedding dimension')
    parser.add_argument('-w', '--wordemb', default=12, type=int, help='opinion embedding dimension')
    parser.add_argument('--data_path', default='../../data/final-obj/Electronics_dataset_obj.pickle', type=str, help='data object path')
    parser.add_argument('--dataset', default='Electronics', type=str, help='Dataset Name')
    parser.add_argument('--bpr', default=10, type=float, help='weight of BPR loss')
    parser.add_argument('--reg', default=0.1, type=float, help='weight of Regularization loss')
    parser.add_argument('--bpr_examples', default=50, type=int, help='number of BPR samples per epoch to train')
    parser.add_argument('--mse_examples', default=50, type=int, help='number of MSE samples per epoch to train')
    parser.add_argument('--lr', default=0.1, type=float, help='initial learning rate')
    parser.add_argument('--adv', default=False, type=bool, help='use adv method or not?')
    parser.add_argument('--adv_method', default='random', type=str, help='which adv method')
    parser.add_argument('--eps', default=1, type=float, help='noise level epsilon')
    parser.add_argument("--seed", dest="seed", type=int, default=999, help="seed")
    parser.add_argument("--epochs", dest="epochs", type=int, default=100, help="training epoch")
    parser.add_argument("--print_every", dest="print_every", type=int, default=10, help="evaluation epoch")
    parser.add_argument("--rec_k", dest="rec_k", type=int, default=20, help="length of rec list")
    parser.add_argument("--exp_k", dest="exp_k", type=int, default=50, help="length of expl list")
    parser.add_argument("--model_path", dest="model_path", type=str, default="./logs/",help = 'path to vanilla model')    
    args = parser.parse_args()
    return args


def load_useritemfea(rec_dataset, U_num, I_num, Fnum):
    sps_tensor_useritemf = {}
    overall_rating = np.zeros((U_num,I_num))
    
    print("======================= User-item-feature tensor creation =======================")
    train_u_i_set = {}
    neg_items = {}
    for row in rec_dataset.training_data:
        user = row[0]
        item = row[1]
        label = row[2]
        if label == 1:
            train_u_i_set[(user, item)] = label
        else:
            if user not in neg_items:
                neg_items[user] = []
            neg_items[user].append(item)

    
    for _,row in (enumerate(rec_dataset.sentiment_data)):
        user = row[0]
        item = row[1]
        
        # Only training data for these tensors creation
        if (user, item) in train_u_i_set:
            label = train_u_i_set[(user, item)]
            
            for fos in row[2:]:
                feature = fos[0]
                senti = int(fos[2])
                
                if (user,item,feature) not in sps_tensor_useritemf:
                    sps_tensor_useritemf[(user,item,feature)] = 0
                sps_tensor_useritemf[(user,item,feature)] += senti
            
            overall_rating[user, item] = label
            sps_tensor_useritemf[(user, item, F_num)] = label
    
    
    
    return overall_rating, sps_tensor_useritemf, list(train_u_i_set.keys()),neg_items


def load_useritemfeaword(rec_dataset, F_num, W_num):
    sps_tensor_userwordf = {}
    sps_tensor_itemwordf = {}
    word_map = {}
    word_count = 0
    # for these tensors use all the data for creation
    for _,row in (enumerate(rec_dataset.sentiment_data)):
        user = row[0]
        item = row[1]
        for fos in row[2:]:
            feature = fos[0]
            opinion = fos[1]
            senti = int(fos[2])
            
            if opinion not in word_map:
                word_map[opinion] = word_count
                word_count += 1
            
            
            # only positive sentiments
            if senti > 0:
                
                if (user,feature,opinion) not in sps_tensor_userwordf:
                    sps_tensor_userwordf[(user,feature,word_map[opinion])] = 0
                sps_tensor_userwordf[(user,feature,word_map[opinion])] += 1
                
                if (item,feature,opinion) not in sps_tensor_itemwordf:
                    sps_tensor_itemwordf[(item,feature,word_map[opinion])] = 0
                sps_tensor_itemwordf[(item,feature,word_map[opinion])] += 1
    
    
    return sps_tensor_userwordf, sps_tensor_itemwordf,word_map


if __name__ == '__main__':

    args = args_process()
    print("Args:",args)
    outfile = 'mter_paraserver' + "_" + str(args.dataset).upper()
    with open(args.data_path,'rb') as handle:
        rec_dataset = pickle.load(handle)
    if args.adv:
        outfile += "_" + str(args.adv_method.upper()) + "_" + str(args.eps)
    
    
    
    
    print("DATASET:",args.dataset.upper())
    rec_dataset.printDetails()
    
    rec_k = int(args.rec_k)
    exp_k = int(args.exp_k)
    
    U_num = rec_dataset.user_num
    I_num = rec_dataset.item_num
    F_num = rec_dataset.feature_num  #+1 including overall rating
    W_num = rec_dataset.opinions_num 
    
    seed = int(args.seed)
    random.seed(seed)
    np.random.seed(seed)
    
    
    
    
    print('Preparing Data...')
    #load the interactions between user, item and feature
    overall_rating_trn, sps_tensor_useritemf_trn, useritem_ls_trn,neg_items = \
            load_useritemfea(rec_dataset, U_num, I_num, F_num)

    for key in sps_tensor_useritemf_trn.keys():
        if key[2] != F_num:
            sps_tensor_useritemf_trn[key] = 1 + 4/(1 + np.exp(0 - sps_tensor_useritemf_trn[key]))

    print('Preparing Data...')
    #load the interactions between user, item, feature and opinion word
    sps_tensor_userwordf_trn, sps_tensor_itemwordf_trn,word_map = \
            load_useritemfeaword(rec_dataset,F_num,W_num)

    for key in sps_tensor_userwordf_trn.keys():
            sps_tensor_userwordf_trn[key] = 1 + 4 * ( 2 / (1 + np.exp(0 - sps_tensor_userwordf_trn[key]))-1)

    for key in sps_tensor_itemwordf_trn.keys():
            sps_tensor_itemwordf_trn[key] = 1 + 4 * ( 2 / (1 + np.exp(0 - sps_tensor_itemwordf_trn[key]))-1)
    
    os.makedirs('logs/', exist_ok=True)
    

    U_dim = args.useremb
    I_dim = args.itememb
    F_dim = args.featemb
    W_dim = args.wordemb
    lmd_BPR = args.bpr
    ld_reg = args.reg
    num_iter = args.epochs
    bpr_sample = args.bpr_examples
    mse_sample = args.mse_examples
    rec_k = int(args.rec_k)
    print_every = int(args.print_every)
    lr = args.lr
    
    if args.adv and args.adv_method.upper() == 'RANDOM':
        with open(args.model_path,'rb') as handle:
            params = pickle.load(handle)
        
        eps = float(args.eps)
        U,I,F,W = params['U'],params['I'],params['F'],params['W']
        G1 = params['G1']
        print("Initial metrics for all users")
        test(rec_dataset.test_data,G1, U, I, F,rec_k = rec_k, val = True)
        
        Unoise = np.random.normal(loc = 0, scale = 1, size = U.shape)
        Inoise = np.random.normal(loc = 0, scale = 1, size = I.shape)
        Fnoise = np.random.normal(loc = 0, scale = 1, size = F.shape)
        Wnoise = np.random.normal(loc = 0, scale = 1, size = W.shape)
        
        Unoise = normalize(Unoise,norm = 'l2', axis = 1)
        Inoise = normalize(Inoise,norm = 'l2', axis = 1)
        Fnoise = normalize(Fnoise,norm = 'l2', axis = 1)
        Wnoise = normalize(Wnoise,norm = 'l2', axis = 1)
        
        Unoise *= eps
        Inoise *= eps
        Fnoise *= eps
        Wnoise *= eps
        
        
        print("Norm of the U noise:",np.linalg.norm(Unoise,axis = 1))
        print("Norm of the I noise:",np.linalg.norm(Inoise,axis = 1))
        print("Norm of the F noise:",np.linalg.norm(Fnoise,axis = 1))
        print("Norm of the W noise:",np.linalg.norm(Wnoise,axis = 1))
        
        print("After adding noise: ")
        
        Uf = U + Unoise
        If = I + Inoise
        Ff = F + Fnoise
        
        Uf[Uf < 0] = 0
        If[If < 0] = 0
        Ff[Ff < 0] = 0
        
        test(rec_dataset.test_data,G1, Uf, If,Ff,rec_k = rec_k, val = True)
        
        noise_params = params.copy()
        noise_params['Unoise'] = Unoise
        noise_params['Inoise'] = Inoise
        noise_params['Fnoise'] = Fnoise
        noise_params['Wnoise'] = Wnoise
        with open('logs/' + outfile + '.paras', 'wb') as output:
            pickle.dump(noise_params, output) 
        
        sys.exit()
    
    
    G1init = np.random.rand(U_dim * I_dim * F_dim)
    G2init = np.random.rand(U_dim * F_dim * W_dim)
    G3init = np.random.rand(I_dim * F_dim * W_dim)
    Uinit = np.random.rand(U_num * U_dim)
    Iinit = np.random.rand(I_num * I_dim)
    Finit = np.random.rand((F_num + 1) * F_dim)
    Winit = np.random.rand(W_num * W_dim)
    
    print("Number of UIF samples:",len(sps_tensor_useritemf_trn))
    print("Number of UFW samples:",len(sps_tensor_userwordf_trn))
    print("Number of IFW samples:",len(sps_tensor_itemwordf_trn))
    print("Number of BPR samples:",len(useritem_ls_trn))
    
    print('Training started! Estimated time is being printed during training ...')
    (G1,G2,G3,U,I,F,W) = tsmtr.train(overall_rating_trn, neg_items,rec_dataset.test_data,
                     sps_tensor_useritemf_trn, sps_tensor_userwordf_trn, sps_tensor_itemwordf_trn, 
                     useritem_ls_trn, U_dim, I_dim, F_dim, W_dim, U_num, I_num, F_num, W_num, 
                     lmd_BPR, ld_reg, rec_k,num_iter, print_every,outfile, lr=lr, params = [Uinit,Iinit,Finit,Winit,G1init,G2init,G3init],random_seed=seed, eps=1e-8,num_samples_bpr = bpr_sample,num_samples_mse = mse_sample,dataset = args.dataset)

    params = {'G1': G1, 'G2': G2, 'G3': G3, 'U':U, 'I':I, 'F':F, 'W':W}
    
    with open('logs/' + outfile + '.paras', 'wb') as output:
        pickle.dump(params, output) 
    
    