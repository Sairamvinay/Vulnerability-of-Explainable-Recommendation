import torch
from torch.autograd import grad
import numpy as np
import os
import sys
import tqdm
import pickle
from pathlib import Path
from torch.utils.data import DataLoader
sys.path.append("../../baselines/")
from data_loaders import UserItemInterwithFeatureDataset
sys.path.append("../../data/")
from text_processed_dataset import TextProcessedDataset
from yelp_processed_dataset import YelpProcessedDataset
from args_process import *
from eval_cer import *
print("TORCH Version: ",torch.__version__)

def seed_worker(worker_id):
    worker_seed = torch.initial_seed() % 2**32
    numpy.random.seed(worker_seed)
    random.seed(worker_seed)
    

class BaseRecModel(torch.nn.Module):
    def __init__(self, feature_length,adv = False):
        super(BaseRecModel, self).__init__()
        self.fc0 = torch.nn.Parameter(torch.randn(feature_length * 2, 1024))
        self.fc2 = torch.nn.Parameter(torch.randn(1024,256))
        self.fc4 = torch.nn.Parameter(torch.randn(256, 1))
        self.fcbias0 = torch.nn.Parameter(torch.randn(1024,1))
        self.fcbias2 = torch.nn.Parameter(torch.randn(256,1))
        self.fcbias4  = torch.nn.Parameter(torch.randn(1))
        torch.nn.init.xavier_normal_(self.fc0)
        torch.nn.init.xavier_normal_(self.fc2)
        torch.nn.init.xavier_normal_(self.fc4)
        self.noise1 = torch.nn.Parameter(torch.zeros(feature_length * 2, 1024)) # the error terms
        self.noise2 = torch.nn.Parameter(torch.zeros(1024, 256)) # the error terms
        self.noise3 = torch.nn.Parameter(torch.zeros(256, 1)) # the error terms

    def forward(self, user_feature, item_feature,adv = False):

        fusion = torch.cat((user_feature, item_feature), 1)
        out1 = (fusion @ (self.fc0 + self.noise1)) + self.fcbias0.t()
        relu1 = torch.nn.ReLU()
        final1 = relu1(out1)
        out2 = (final1  @ (self.fc2 + self.noise2)) + self.fcbias2.t()
        relu2 = torch.nn.ReLU()
        final2 = relu2(out2)
        out3 = (final2  @(self.fc4 + self.noise3)) + self.fcbias4.t()
        sigm = torch.nn.Sigmoid()
        out = sigm(out3)
            
        return out

def model_grad(model):
    with torch.no_grad():
        for name, param in model.named_parameters():
            print("Name: ",name, end = ',')
            if param.requires_grad:
                print("Param needs grad: which is",param.grad)        


def weight_model(model):
    with torch.no_grad():
        for name, param in model.named_parameters():
            print("Name:",name,': Param weight:',param, 'param needs grad:',param.requires_grad)
           

             
def train_base_recommendation(train_args,rec_dataset,adv = False,adv_method = 'fgsm',eps = 0.5,verbose = False):
    if train_args.gpu:
        device = torch.device('cuda:%s' % train_args.cuda)
    else:
        device = 'cpu'
    
    g = torch.Generator()
    g.manual_seed(train_args.seed)
    train_loader = DataLoader(dataset=UserItemInterwithFeatureDataset(rec_dataset.training_data, 
                                rec_dataset.user_feature_matrix, 
                                rec_dataset.item_feature_matrix),
                          batch_size=train_args.batch_size,
                          shuffle=True,worker_init_fn=seed_worker,generator=g)

    model = BaseRecModel(rec_dataset.feature_num,adv= adv).to(device)
    print("Model Params: ")
    model_grad(model)
    
    if adv:
        
        # model.load_state_dict(torch.load(os.path.join(train_args.base_model_path, train_args.dataset.upper()+"_logs", "model.model")))
        model.load_state_dict(torch.load(train_args.base_model_path))
        # make sure you freeze the parameters
        trainable_params = []
        with torch.no_grad():
            for name,param in model.named_parameters():
                if name[:5] == 'noise':
                    print("Name of trainable param:",name)
                    trainable_params.append(param)
                    param.requires_grad = True
                    continue
                
                if name[:5] != 'noise' and param.requires_grad:
                    print("Frozen params: ", name)
                    param.requires_grad = False
       
    else:
        trainable_params = []
        with torch.no_grad():
            for name,param in model.named_parameters():
                if name[:5] == 'noise':
                    print("Frozen params: ", name)
                    param.requires_grad = False
                    continue
                
                if name[:5] != 'noise' and param.requires_grad:
                    trainable_params.append(param)
                    print("Name of trainable param:",name)
                    param.requires_grad = True
        
    if adv:
        print("MODEL LOADED Weights:")
        weight_model(model)
        
    
    
    loss_fn = torch.nn.BCELoss()
    optimizer = torch.optim.SGD(trainable_params, lr=train_args.lr, weight_decay=train_args.weight_decay)
    name = train_args.dataset.upper()
    if adv:
        name += "_"+adv_method + "_" + str(train_args.eps)
    out_path = os.path.join("./logs", name + "_logs")
    Path(out_path).mkdir(parents=True, exist_ok=True)

    all_losses = [] 
    ndcgs = []
    precisions = []
    recalls = []
    f1s = []
    batch = 0
    top_k = train_args.rec_k
    
    print("Initial Metrics")
#     init_ndcg = compute_ndcg(rec_dataset.test_data, rec_dataset.user_feature_matrix, rec_dataset.item_feature_matrix, top_k, model,device,adv= adv)

#     init_recall,init_precision,init_f1 = compute_precision_recall(rec_dataset.test_data, 
#             rec_dataset.user_feature_matrix, 
#             rec_dataset.item_feature_matrix, 
#             top_k, 
#             model, 
#             device,adv= adv)

    init_ndcg,init_precision,init_recall,init_f1 = compute_metrics(rec_dataset.test_data, rec_dataset.user_feature_matrix, rec_dataset.item_feature_matrix, top_k, model,device,adv= adv)
#     sys.exit()
        
    if adv and adv_method.upper() == "RANDOM":
        with torch.no_grad():
            noise1 = torch.randn(model.noise1.size())
            noise2 = torch.randn(model.noise2.size())
            noise3 = torch.randn(model.noise3.size())
            

            noise1 /= max(torch.norm(noise1, p = 2), 1e-12)
            noise2 /= max(torch.norm(noise2, p = 2), 1e-12)
            noise3 /= max(torch.norm(noise3, p = 2), 1e-12)

            model.noise1.copy_(eps * noise1)
            model.noise2.copy_(eps * noise2)
            model.noise3.copy_(eps * noise3)
        print("After adding random noise")
#         ndcg = compute_ndcg(rec_dataset.test_data, 
#                 rec_dataset.user_feature_matrix, 
#                 rec_dataset.item_feature_matrix, 
#                 top_k, 
#                 model, 
#                 device,adv= adv)

#         recall,precision,f1 = compute_precision_recall(rec_dataset.test_data, 
#         rec_dataset.user_feature_matrix, 
#         rec_dataset.item_feature_matrix, 
#         top_k, 
#         model, 
#         device,adv= adv)

        ndcg,precision,recall,f1 = compute_metrics(rec_dataset.test_data, rec_dataset.user_feature_matrix, rec_dataset.item_feature_matrix, top_k, model,device,adv= adv)
        
        print("After training: MODEL LOADED Weights:")
        weight_model(model)
        torch.save(model.state_dict(), os.path.join(out_path, "model.model"))
        return 0
    
    
    else:
        for epoch in tqdm.trange(1, 1 + train_args.epoch):
            model.train()
            losses = []
            optimizer.zero_grad()
            for user_behaviour_feature, item_aspect_feature, label in train_loader:
                optimizer.zero_grad()
                user_behaviour_feature = user_behaviour_feature.to(device)
                item_aspect_feature = item_aspect_feature.to(device)
                label = label.float().to(device)
                
                out = model(user_behaviour_feature, item_aspect_feature,adv = adv).squeeze()
                loss = loss_fn(out, label)
                loss.backward(retain_graph = True)
                
                if adv and adv_method.upper() == 'FGSM':
                    
                    grads = torch.autograd.grad(loss, [model.noise1, model.noise2, model.noise3])
                    grad1,grad2,grad3 = grads[0],grads[1],grads[2]
                    grad_delta1 = grad1 / max(torch.norm(grad1, p = 2),1e-12)
                    grad_delta2 = grad2 / max(torch.norm(grad2, p = 2),1e-12)
                    grad_delta3  = grad3 /  max(torch.norm(grad3, p = 2),1e-12)
                    
                    if torch.sum(grad1) == 0 or torch.sum(grad2) == 0 or torch.sum(grad3) == 0:
                        print("BATCH ",batch," has 0 gradient")
                        # print(" GRAD  Noise 1:",torch.sum(grad1), " GRAD Grad 2:",torch.sum(grad2), " GRAD  Grad 3:",torch.sum(grad3))
                        
                    with torch.no_grad():
                        model.noise1.copy_(eps * grad_delta1)
                        model.noise2.copy_(eps * grad_delta2)
                        model.noise3.copy_(eps * grad_delta3)
                    

                else:
                    pass
                
                optimizer.step()
                
                batch += 1
                
                losses.append(loss.to('cpu').detach().numpy())
                ave_train = np.mean(np.array(losses))
                if verbose:
                    print("Epoch ",epoch, 'Batch ',batch, ' Total loss: ',losses[-1], ' Model output:',torch.sum(out))

            all_losses.append(round(ave_train,6))
            print('epoch %d: ' % epoch, 'training loss: ', round(ave_train,6))
            if epoch == 1 or (epoch % 5 == 0):
                torch.save(model.state_dict(), os.path.join(out_path, "model.model"))
            
            # compute ndcg
            if (epoch == train_args.epoch) or (epoch % train_args.eval == 0):
                
                    
#                 ndcg = compute_ndcg(rec_dataset.test_data, 
#                 rec_dataset.user_feature_matrix, 
#                 rec_dataset.item_feature_matrix, 
#                 top_k, 
#                 model, 
#                 device,adv= adv)

#                 recall,precision,f1 = compute_precision_recall(rec_dataset.test_data, 
#                 rec_dataset.user_feature_matrix, 
#                 rec_dataset.item_feature_matrix, 
#                 top_k, 
#                 model, 
#                 device,adv= adv)
                ndcg,precision,recall,f1 = compute_metrics(rec_dataset.test_data, rec_dataset.user_feature_matrix, rec_dataset.item_feature_matrix, top_k, model,device,adv= adv)

                ndcgs.append(ndcg)
                precisions.append(precision)
                recalls.append(recall)
                f1s.append(f1)


        if adv:
            print("After training: MODEL LOADED Weights:")
            weight_model(model)

        torch.save(model.state_dict(), os.path.join(out_path, "model.model"))
        plot_metrics(all_losses,metric_name = 'Loss', dataset = train_args.dataset,adv=adv,adv_method = adv_method,eps = str(train_args.eps),model_name='CER-Base')
        plot_metrics(ndcgs,metric_name = 'NDCG@'+str(top_k), dataset = train_args.dataset, adv=adv,adv_method = adv_method,eps=str(train_args.eps),model_name='CER-Base')
        plot_metrics(precisions,metric_name = 'Precision@'+str(top_k), dataset = train_args.dataset,adv=adv,adv_method = adv_method,eps=str(eps),model_name='CER-Base')
        plot_metrics(recalls,metric_name = 'Recall@'+str(top_k),  dataset = train_args.dataset,adv=adv,adv_method = adv_method,eps=str(eps),model_name='CER-Base')
        plot_metrics(f1s,metric_name = 'F1@'+str(top_k), dataset = train_args.dataset,adv=adv,adv_method = adv_method,eps=str(eps),model_name='CER-Base')

        return 0


if __name__ == "__main__":
    # Parse argument
    
    t_args = arg_parse_train_base()  # training arguments
    torch.manual_seed(t_args.seed)
    np.random.seed(t_args.seed)
    print("ARGS:",t_args)
    if t_args.gpu:
        os.environ["CUDA_VISIBLE_DEVICES"] = t_args.cuda
        print("Using CUDA", t_args.cuda)
    else:
        print("Using CPU")
    
    with open(t_args.data_path, 'rb') as handle:
        rec_dataset = pickle.load(handle)
    
    print("Dataset: ",t_args.dataset.upper())
    rec_dataset.printDetails()
    if t_args.adv:
        print("#"*50, "ADVERSARIAL TRAINING NOW-",t_args.adv_method.upper(), "#"*50)
    else:
        print("#"*50, "SIMPLE TRAINING NOW", "#"*50)
    train_base_recommendation(t_args, rec_dataset,adv = t_args.adv,adv_method = t_args.adv_method,eps = t_args.eps,verbose = 0)
