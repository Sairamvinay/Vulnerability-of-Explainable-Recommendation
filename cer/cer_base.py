import torch
import numpy as np
import os
import sys
import tqdm
import pickle
from pathlib import Path
from torch.utils.data import DataLoader
sys.path.append("../baselines/")
from data_loaders import UserItemInterwithFeatureDataset
sys.path.append("../data/")
from text_processed_dataset import TextProcessedDataset
from yelp_processed_dataset import YelpProcessedDataset
from args_process import *
from eval_cer import *
print("TORCH Version: ",torch.__version__)

def seed_worker(worker_id):
    worker_seed = torch.initial_seed() % 2**32
    numpy.random.seed(worker_seed)
    random.seed(worker_seed)
    

class BaseRecModel(torch.nn.Module):
    def __init__(self, feature_length,adv = False):
        super(BaseRecModel, self).__init__()
        self.fc0 = torch.nn.Parameter(torch.randn(feature_length * 2, 1024))
        self.fc2 = torch.nn.Parameter(torch.randn(1024,256))
        self.fc4 = torch.nn.Parameter(torch.randn(256, 1))
        self.fcbias0 = torch.nn.Parameter(torch.randn(1024,1))
        self.fcbias2 = torch.nn.Parameter(torch.randn(256,1))
        self.fcbias4  = torch.nn.Parameter(torch.randn(1))
        torch.nn.init.xavier_normal_(self.fc0)
        torch.nn.init.xavier_normal_(self.fc2)
        torch.nn.init.xavier_normal_(self.fc4)
        self.noise1 = torch.nn.Parameter(torch.zeros(feature_length * 2, 1024)) # the error terms
        self.noise2 = torch.nn.Parameter(torch.zeros(1024, 256)) # the error terms
        self.noise3 = torch.nn.Parameter(torch.zeros(256, 1)) # the error terms

    def forward(self, user_feature, item_feature,adv = False):

        fusion = torch.cat((user_feature, item_feature), 1)
        
        if adv:
            out1 = (fusion @(self.fc0)) + (fusion @ self.noise1) + self.fcbias0.t()
            relu1 = torch.nn.ReLU()
            final1 = relu1(out1)
            out2 = (final1  @ (self.fc2)) + (final1 @ self.noise2) + self.fcbias2.t()
            relu2 = torch.nn.ReLU()
            final2 = relu2(out2)
            out3 = (final2  @(self.fc4)) + (final2 @ self.noise3) + self.fcbias4.t()
#             print("OUT 1:",final1)
#             print("OUT 2:",final2)
#             print("OUT 3:",out3)
            sigm = torch.nn.Sigmoid()
            out = sigm(out3)
        
        else:
            out1 = (fusion @(self.fc0)) + self.fcbias0.t()
            relu1 = torch.nn.ReLU()
            final1 = relu1(out1)
            out2 = (final1  @(self.fc2)) + self.fcbias2.t()
            relu2 = torch.nn.ReLU()
            final2 = relu2(out2)
            out3 = (final2  @(self.fc4)) + self.fcbias4.t()
            sigm = torch.nn.Sigmoid()
            out = sigm(out3)    
            
        return out

def model_grad(model):
    with torch.no_grad():
        for name, param in model.named_parameters():
            print("Name: ",name, end = ',')
            if param.requires_grad:
                print("Param needs grad: which is",param.grad)        


def weight_model(model):
    with torch.no_grad():
        for name, param in model.named_parameters():
            print("Name:",name,': Param weight:',param, 'param needs grad:',param.requires_grad)
           

             
def train_base_recommendation(train_args,rec_dataset,adv = False,adv_method = 'fgsm',eps = 0.5,verbose = False):
    if train_args.gpu:
        device = torch.device('cuda:%s' % train_args.cuda)
    else:
        device = 'cpu'
    
    g = torch.Generator()
    g.manual_seed(train_args.seed)
    train_loader = DataLoader(dataset=UserItemInterwithFeatureDataset(rec_dataset.training_data, 
                                rec_dataset.user_feature_matrix, 
                                rec_dataset.item_feature_matrix),
                          batch_size=train_args.batch_size,
                          shuffle=True,worker_init_fn=seed_worker,generator=g)

    model = BaseRecModel(rec_dataset.feature_num,adv= adv).to(device)
    print("Model Params: ")
    model_grad(model)
    
    if adv:
        
        model.load_state_dict(torch.load(os.path.join(train_args.base_model_path, train_args.dataset.upper()+"_logs", "model.model")))
        # make sure you freeze the parameters
        trainable_params = []
        with torch.no_grad():
            for name,param in model.named_parameters():
                if name[:5] == 'noise':
                    print("Name of trainable param:",name)
                    trainable_params.append(param)
                    param.requires_grad = True
                    continue
                
                if name[:5] != 'noise' and param.requires_grad:
                    print("Frozen params: ", name)
                    param.requires_grad = False
       
    else:
        trainable_params = model.parameters()
        
    if adv:
        print("MODEL LOADED Weights:")
        weight_model(model)
    
    loss_fn = torch.nn.BCELoss()
    optimizer = torch.optim.SGD(trainable_params, lr=train_args.lr, weight_decay=train_args.weight_decay)
    
    name = train_args.dataset.upper()
    if adv:
        name += "_"+adv_method + "_" + str(train_args.eps)
    out_path = os.path.join("./logs", name + "_logs")
    Path(out_path).mkdir(parents=True, exist_ok=True)

    
#     for k in [5, 10, 20, 50, 100]:
#         init_ndcg = compute_ndcg(rec_dataset.test_data, rec_dataset.user_feature_matrix, rec_dataset.item_feature_matrix, k, model,device,adv= adv)

#         init_recall,init_precision = compute_precision_recall(rec_dataset.test_data, 
#                 rec_dataset.user_feature_matrix, 
#                 rec_dataset.item_feature_matrix, 
#                 k, 
#                 model, 
#                 device,adv= adv)
        
#         if (init_recall + init_precision) == 0:
#             init_f1 = 0
#         else:
#             init_f1 = (2 * init_precision * init_recall) / (init_recall + init_precision)

#         print("Init NDCG@",k,":",round(init_ndcg,6),' Init Precision@',k,":", round(init_precision,6), ' Init Recall@',k,":", round(init_recall,6),' Init F1@',k,":",round(init_f1,6))
    all_losses = [] 
    ndcgs = []
    precisions = []
    recalls = []
    f1s = []
    batch = 0
    top_k = train_args.rec_k
    
    if adv and adv_method.upper() == "RANDOM":
        with torch.no_grad():
            noise1 = torch.randn(model.noise1.size())
            noise2 = torch.randn(model.noise2.size())
            noise3 = torch.randn(model.noise3.size())
            
            noise1 /= max(torch.norm(noise1, p = 2), 1e-12)
            noise2 /= max(torch.norm(noise2, p = 2), 1e-12)
            noise3 /= max(torch.norm(noise3, p = 2), 1e-12)

            model.noise1.copy_(eps * noise1)
            model.noise2.copy_(eps * noise2)
            model.noise3.copy_(eps * noise3)
        print("After adding random noise")
        for k in [5,10,20,50,100]:
            ndcg = compute_ndcg(rec_dataset.test_data, 
            rec_dataset.user_feature_matrix, 
            rec_dataset.item_feature_matrix, 
            k, 
            model, 
            device,adv= adv)

            recall,precision = compute_precision_recall(rec_dataset.test_data, 
            rec_dataset.user_feature_matrix, 
            rec_dataset.item_feature_matrix, 
            k, 
            model, 
            device,adv= adv)

            ndcg = round(ndcg,6)
            precision = round(precision,6)
            recall = round(recall,6)
            if (recall + precision) == 0:
                f1 = 0.0
            else:
                f1 = (2 * precision * recall) / (recall + precision)

            f1 = round(f1,6)
            if k == top_k:
                ndcgs.append(ndcg)
                precisions.append(precision)
                recalls.append(recall)
                f1s.append(f1)

            print("\t\t", 'NDCG@',k,":", ndcg,'Precision@',k,":", precision, 'Recall@',k,":", recall," F1@",k,":",f1)
        
        print("After training: MODEL LOADED Weights:")
        weight_model(model)
        torch.save(model.state_dict(), os.path.join(out_path, "model.model"))
        return 0
    
    
    else:
        for epoch in tqdm.trange(1, 1 + train_args.epoch):
            model.train()
            optimizer.zero_grad()
            losses = []

            for user_behaviour_feature, item_aspect_feature, label in train_loader:
                user_behaviour_feature = user_behaviour_feature.to(device)
                item_aspect_feature = item_aspect_feature.to(device)
                label = label.float().to(device)


                out = model(user_behaviour_feature, item_aspect_feature,adv = adv).squeeze()

                if adv:

                    loss = -loss_fn(out, label)
                    loss.backward(retain_graph = True)
                else:
                    loss = loss_fn(out, label)
                    loss.backward()

                optimizer.step()
                batch += 1
                if batch > 0 and adv and adv_method.upper() == 'FGSM':
                    with torch.no_grad():
                        
                        grad_delta1 = model.noise1.grad
                        grad_delta2 = model.noise2.grad
                        grad_delta3 = model.noise3.grad
                        
                        grad_delta1 /= max(torch.norm(grad_delta1, p = 2),1e-12)
                        grad_delta2 /= max(torch.norm(grad_delta2, p = 2),1e-12)
                        grad_delta3 /= max(torch.norm(grad_delta3, p = 2),1e-12)
                        

                        
                        if torch.sum(model.noise1.grad) == 0 or torch.sum(model.noise2.grad)  == 0 or torch.sum(model.noise3.grad) == 0:
                            print("BATCH ",batch," has 0 gradient")                            
                        
                        
                        model.noise1.copy_(eps * grad_delta1)
                        model.noise2.copy_(eps * grad_delta2)
                        model.noise3.copy_(eps * grad_delta3)
                        
                        

                else:
                    pass
                optimizer.zero_grad()
                losses.append(loss.to('cpu').detach().numpy())
                ave_train = np.mean(np.array(losses))
                if verbose:
                    print("Epoch ",epoch, 'Batch ',batch, ' Total loss: ',losses[-1], ' Model output:',torch.sum(out))

            all_losses.append(round(ave_train,6))
            print('epoch %d: ' % epoch, 'training loss: ', round(ave_train,6))
            # compute ndcg
            if (epoch == train_args.epoch) or (epoch % train_args.eval == 0):
                for k in [5,10,20,50,100]:
                    ndcg = compute_ndcg(rec_dataset.test_data, 
                    rec_dataset.user_feature_matrix, 
                    rec_dataset.item_feature_matrix, 
                    k, 
                    model, 
                    device,adv= adv)

                    recall,precision = compute_precision_recall(rec_dataset.test_data, 
                    rec_dataset.user_feature_matrix, 
                    rec_dataset.item_feature_matrix, 
                    k, 
                    model, 
                    device,adv= adv)

                    ndcg = round(ndcg,6)
                    precision = round(precision,6)
                    recall = round(recall,6)
                    if (recall + precision) == 0:
                        f1 = 0.0
                    else:
                        f1 = (2 * precision * recall) / (recall + precision)

                    f1 = round(f1,6)
                    if k == top_k:
                        ndcgs.append(ndcg)
                        precisions.append(precision)
                        recalls.append(recall)
                        f1s.append(f1)

                    print("\t\t", 'NDCG@',k,":", ndcg,'Precision@',k,":", precision, 'Recall@',k,":", recall," F1@",k,":",f1)

        if adv:
            print("After training: MODEL LOADED Weights:")
            weight_model(model)

        torch.save(model.state_dict(), os.path.join(out_path, "model.model"))
        plot_metrics(all_losses,metric_name = 'Loss', dataset = train_args.dataset,adv=adv,adv_method = adv_method,eps = str(train_args.eps),model_name='CER-Base')
        plot_metrics(ndcgs,metric_name = 'NDCG@'+str(top_k), dataset = train_args.dataset, adv=adv,adv_method = adv_method,eps=str(train_args.eps),model_name='CER-Base')
        plot_metrics(precisions,metric_name = 'Precision@'+str(top_k), dataset = train_args.dataset,adv=adv,adv_method = adv_method,eps=str(eps),model_name='CER-Base')
        plot_metrics(recalls,metric_name = 'Recall@'+str(top_k),  dataset = train_args.dataset,adv=adv,adv_method = adv_method,eps=str(eps),model_name='CER-Base')
        plot_metrics(f1s,metric_name = 'F1@'+str(top_k), dataset = train_args.dataset,adv=adv,adv_method = adv_method,eps=str(eps),model_name='CER-Base')

        return 0


if __name__ == "__main__":
    # Parse argument
    
    t_args = arg_parse_train_base()  # training arguments
    torch.manual_seed(t_args.seed)
    np.random.seed(t_args.seed)
    print("ARGS:",t_args)
    if t_args.gpu:
        os.environ["CUDA_VISIBLE_DEVICES"] = t_args.cuda
        print("Using CUDA", t_args.cuda)
    else:
        print("Using CPU")
    
    with open(t_args.data_path, 'rb') as handle:
        rec_dataset = pickle.load(handle)
    
    print("Dataset: ",t_args.dataset.upper())
    rec_dataset.printDetails()
    if t_args.adv:
        print("#"*50, "ADVERSARIAL TRAINING NOW-",t_args.adv_method.upper(), "#"*50)
    else:
        print("#"*50, "SIMPLE TRAINING NOW", "#"*50)
    train_base_recommendation(t_args, rec_dataset,adv = t_args.adv,adv_method = t_args.adv_method,eps = t_args.eps,verbose = 0)
