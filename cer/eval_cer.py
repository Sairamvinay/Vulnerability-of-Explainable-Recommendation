import numpy as np
import torch
from sklearn.metrics import ndcg_score
import matplotlib.pyplot as plt
import os
def plot_metrics(metrics, metric_name = 'Loss',dataset = "Electronics",adv = False, adv_method = 'fgsm',eps = 0.5, multi = False,names = None,model_name = 'CER-BASE'):
    epochs = list(range(1 , 1 + len(metrics)))
    image_name = model_name+"-plots/" + dataset + "_" + metric_name
    if adv:
        image_name += '_ADV'+"_"+str(adv_method)+"_"+str(eps)
    dirname = os.path.dirname(os.path.abspath(image_name))
    os.makedirs(dirname, exist_ok=True)
    plt.figure(figsize = (12,8))
    plt.title(model_name + "-" + dataset + "-" + metric_name + ' vs Epochs')
    
    if multi:
        
        for name,metric in zip(names,metrics):
            epochs = list(range(1 , 1 + len(metric)))
            plt.plot(epochs, metric, label = name)
    else:
        plt.plot(epochs, metrics)
    
    
    plt.xlabel("Epochs")
    plt.ylabel(metric_name)
    if multi:
        plt.legend()
    
    plt.savefig(image_name + '.png') 
    
def compute_precision_recall(test_data, user_feature_matrix,item_feature_matrix,rec_k,model,device, adv = False):
    model.eval()
    
    precisions = {5:[],10:[],20:[],50:[],100:[]}
    recalls = {5:[],10:[],20:[],50:[],100:[]}
    top_k_items = []
    Ks = [5,10,20,50,100]
    max_K = max(Ks)
    top_k = {}
    num_pop = 0
    den_pop = 0
    with torch.no_grad():
        
        for row in test_data:
            user = row[0]
            items = row[1]
            gt_labels = row[2]
            
            user_features = np.array([user_feature_matrix[user] for i in range(len(items))])
            item_features = np.array([item_feature_matrix[item] for item in items])
            scores = model(torch.from_numpy(user_features).to(device),
                                    torch.from_numpy(item_features).to(device),adv=adv).squeeze()
            
            scores = np.array(scores.to('cpu'))
            
            triples = [(item,ground,score) for item,ground,score in list(zip(items,gt_labels,scores))]
        
            triples.sort(key = lambda x: x[2],reverse= True)

            final_triples = triples[:max_K]
            top_k_items.append([item for item,_,_ in final_triples])
            top_k[user] = final_triples
            
        
        for row in test_data:
            user = row[0]
            items = row[1]
            triple = top_k[user]
            for k in Ks:
                curr_triple = triple[:k]
                items_recommended,gt_k,score_k = list(zip(*curr_triple))
                items_relevant = [item for item,score,label in zip(items_recommended, score_k,gt_k) if (score >= 0.5 and label == 1) or (score < 0.5 and label == 0)]

                common = set(items) & set(items_relevant)
                common_size = len(common)
                recalls[k].append(common_size / len(items))
                precisions[k].append(common_size / min([k, len(items)]))

    returns = ()
    for k in Ks:
        recall = np.mean(recalls[k])
        precision = np.mean(precisions[k])
        if precision + recall != 0:
            f1 = 2 * recall * precision / (precision + recall)
        
        else:
            f1 = 0.0
        
        precision = round(precision,6)
        recall = round(recall,6)
        f1 = round(f1,6)
        if k == rec_k:
            returns = (recall, precision, f1)
        print("\t\t", "Precision@",k,":",precision,'Recall@',k,":", recall," F1@",k,":",f1)
        
    return returns

def compute_ndcg(test_data, user_feature_matrix, item_feature_matrix, rec_k, model, device,adv = False):
    model.eval()
    
    Ks = [5,10,20,50,100]
    ndcgs= {5:[], 10:[], 20: [], 50: [], 100:[]}
    with torch.no_grad():
        for row in test_data:
            user = row[0]
            items = row[1]
            gt_labels = row[2]
            user_features = np.array([user_feature_matrix[user] for i in range(len(items))])
            item_features = np.array([item_feature_matrix[item] for item in items])
            scores = model(torch.from_numpy(user_features).to(device),
                                    torch.from_numpy(item_features).to(device),adv=adv).squeeze()
            scores = np.array(scores.to('cpu'))
            for k in Ks:
                ndcg = ndcg_score([gt_labels], [scores], k=k)
                ndcgs[k].append(ndcg)
                
    for k in Ks:
        print("\t\t", 'NDCG@',k,":", round(np.mean(ndcgs[k]),6))
    
    return round(np.mean(ndcgs[rec_k]),6)

def evaluate_user_perspective(user_perspective_data, u_i_expl_dict):
    pres = []
    recs = []
    f1s = []
    for u_i, gt_features in user_perspective_data.items():
        if u_i in u_i_expl_dict:
            TP = 0
            pre_features = u_i_expl_dict[u_i]
            print('f: ground: ', gt_features, ' predicted: ',pre_features)
            for feature in pre_features:
                if feature in gt_features:
                    TP += 1
            pre = TP / max(len(pre_features),1)
            rec = TP / max(len(gt_features),1)
            if (pre + rec) != 0:
                f1 = (2 * pre * rec) / (pre + rec)
            else:
                f1 = 0
            pres.append(pre)
            recs.append(rec)
            f1s.append(f1)
    ave_pre = np.mean(pres)
    ave_rec = np.mean(recs)
    ave_f1 = np.mean(f1s)
    return ave_pre, ave_rec, ave_f1


def compute_metrics(test_data, user_feature_matrix,item_feature_matrix,rec_k,model,device, adv = False):
    model.eval()
    
    Ks = [5,10,20,50,100]
    maxK = max(Ks)
    top_K = {}
    with torch.no_grad():
        for row in test_data:
            user = row[0]
            items = row[1]
            gt_labels = row[2]
            user_features = np.array([user_feature_matrix[user] for i in range(len(items))])
            item_features = np.array([item_feature_matrix[item] for item in items])
            scores = model(torch.from_numpy(user_features).to(device),
                                    torch.from_numpy(item_features).to(device),adv=adv).squeeze()
            scores = np.array(scores.to('cpu'))
            triples = [(item,ground,score) for item,ground,score in list(zip(items,gt_labels,scores))]
            triples.sort(key = lambda x: x[2],reverse= True)
            
            top_K[user] = triples[:maxK]

    all_ns = {}
    all_ps = {}
    all_rs = {}
    all_fs = {}
    for k in Ks:
        ndcgs = []
        precisions = []
        recalls = []
        for i, row in (enumerate(test_data)):
            triple = top_K[row[0]][:k]
            batch_items = row[1]
            items_recommended,gt_k,score_k = zip(*triple)
            items_relevant = [item for item,score,label in zip(items_recommended, score_k,gt_k) if (score >= 0.5 and label == 1) or (score < 0.5 and label == 0)]
            
            ndcgs.append(ndcg_score([gt_k],[score_k],k = k))
            common = set(batch_items) & set(items_relevant)
            common_size = len(common)
            recalls.append(common_size / len(batch_items))
            precisions.append(common_size / min([k, len(batch_items)]))
        
        av_n = round(np.mean(ndcgs),6)
        av_p = round(np.mean(precisions),6)
        av_r = round(np.mean(recalls),6)
        if av_p + av_r == 0:
            av_f = 0
        
        else:
            av_f = 2 * av_p * av_r / (av_p + av_r)
        print("NDCG@",k,":",av_n,end = '\t')
        print("Precision@",k,':',av_p,end = '\t')
        print("Recall@",k,':',av_r,end = '\t')
        print("F1@",k,':',round(av_f,6),end = '\t')
        print()
        all_ns[k] = av_n
        all_ps[k] = av_p
        all_rs[k] = av_r
        all_fs[k] = round(av_f,6)
    
    
    return all_ns[rec_k],all_ps[rec_k], all_rs[rec_k], all_fs[rec_k]    

def evaluate_model_perspective(
        rec_dict,
        u_i_exp_dict,
        base_model,
        user_feature_matrix,
        item_feature_matrix,
        rec_k,
        device,adv=False):
    """
    compute PN, PS and F_NS score for the explanations
    :param rec_dict: {u1: [i1, i2, i3, ...] , u2: [i1, i2, i3, ...]}
    :param u_i_exp_dict: {(u, i): [f1, f2, ...], ...}
    :param base_model: the trained base recommendation model
    :param user_feature_matrix: |u| x |p| matrix, the attention on each feature p for each user u
    :param item_feature_matrix: |i| x |p| matrix, the quality on each feature p for each item i
    :param rec_k: the length of the recommendation list, only generated explanations for the items on the list
    :param device: the device of the model
    :return: the mean of the PN, PS and FNS scores
    """
    pn_count = 0
    ps_count = 0
    
    for u_i, fs in u_i_exp_dict.items():
        user = u_i[0]
        target_item = u_i[1]
        if len(fs) == 0:
            continue #no explanation for this pair
        
        features = set(fs)
        items = rec_dict[user]
        target_index = items.index(target_item)
        # compute PN
        cf_items_features = []
        for item in items:
            item_ori_feature = np.array(item_feature_matrix[item])
            item_cf_feature = np.array([0 if s in features else item_ori_feature[s]
                                        for s in range(len(item_ori_feature))], dtype='float32')
            cf_items_features.append(item_cf_feature)
        cf_ranking_scores = base_model(torch.from_numpy(np.array([user_feature_matrix[user]
                                                                      for i in range(len(cf_items_features))])
                                                            ).to(device),
                                           torch.from_numpy(np.array(cf_items_features)).to(device),adv = adv).squeeze()
        cf_score_list = cf_ranking_scores.to('cpu').detach().numpy()
        sorted_index = np.argsort(cf_score_list)[::-1]
        cf_rank = np.argwhere(sorted_index == target_index)[0, 0]  # the updated ranking of the current item
        if cf_rank > rec_k - 1:
            pn_count += 1
        # compute NS
        cf_items_features = []
        for item in items:
            item_ori_feature = np.array(item_feature_matrix[item])
            item_cf_feature = np.array([item_ori_feature[s] if s in features else 0
                                        for s in range(len(item_ori_feature))], dtype='float32')
            cf_items_features.append(item_cf_feature)
        cf_ranking_scores = base_model(torch.from_numpy(np.array([user_feature_matrix[user]
                                                                      for i in range(len(cf_items_features))])
                                                            ).to(device),
                                           torch.from_numpy(np.array(cf_items_features)).to(device),adv = adv).squeeze()
        cf_score_list = cf_ranking_scores.to('cpu').detach().numpy()
        sorted_index = np.argsort(cf_score_list)[::-1]
        cf_rank = np.argwhere(sorted_index == target_index)[0, 0]  # the updated ranking of the current item
        if cf_rank < rec_k:
            ps_count += 1
    if len(u_i_exp_dict) != 0:
        pn = pn_count / len(u_i_exp_dict)
        ps = ps_count / len(u_i_exp_dict)
        if (pn + ps) != 0:
            fns = (2 * pn * ps) / (pn + ps)
        else:
            fns = 0
    else:
        pn = 0
        ps = 0
        fns = 0
    return pn, ps, fns