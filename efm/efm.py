from __future__ import print_function
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import random
import numpy as np
from sklearn.metrics import ndcg_score
import argparse
import sys
import matplotlib.pyplot as plt
from tqdm import tqdm
from pathlib import Path
import pickle
sys.path.append("../../data")
from text_processed_dataset import TextProcessedDataset
from yelp_processed_dataset import YelpProcessedDataset
os.environ['CUDA_VISIBLE_DEVICES'] = '2'

def args_process():    # Parse argument
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", dest="dataset", type=str, default="electronics",help = 'Name of the dataset')
    parser.add_argument("--resume_train",dest = "resume_train", type = bool, default = False, help = "continue the training")
    parser.add_argument("--print_every",dest = "print_every",type = int, default = 10, help = 'print every x batches once')
    parser.add_argument("--dim", type = int, default = 64, help = 'Hidden Embedding dimensions')
    parser.add_argument("--lr", dest="lr", type=float, default=0.001, help="learning rate for training")
    parser.add_argument("--seed", dest="seed", type=int, default=999, help="seed for training")
    parser.add_argument("--epochs", dest="epochs", type=int, default=100, help="training epoch")
    parser.add_argument("--weight_decay", dest="weight_decay", type=float, default=0.1, help="Weight decay")
    parser.add_argument("--verbose", type = int, default = 0, help = 'Print extra info')
    parser.add_argument("--data_path",dest = 'data_path', type=str, default ='../data/final-obj/Electronics_dataset_obj.pickle',help = "Path to the dataset for loading")
    parser.add_argument("--rec_k", dest="rec_k", type=int, default=20, help="length of rec list")
    parser.add_argument("--exp_k", dest="exp_k", type=int, default=50, help="length of expl list")
    parser.add_argument("--alpha", dest="alpha", type=float, default=0.85, help="ratio for the personalized recommendation")
    parser.add_argument("--model_path", dest="model_path", type=str, default="./logs/",help = 'path to vanilla model')
    args = parser.parse_args()
    return args

def plot_metrics(metrics, metric_name = 'Loss',dataset = "Electronics",adv = False, adv_method = 'fgsm',eps = 0.5, multi = False,names = None,model_name = 'EFM'):
    epochs = list(range(1 , 1 + len(metrics)))
    image_name = model_name+"-plots/" + dataset + "_" + metric_name
    if adv:
        image_name += '_ADV'+"_"+str(adv_method)+"_"+str(eps)
    dirname = os.path.dirname(os.path.abspath(image_name))
    os.makedirs(dirname, exist_ok=True)
    plt.figure(figsize = (12,8))
    plt.title(model_name + "-" + dataset + "-" + metric_name + ' vs Epochs')
    
    if multi:
        
        for name,metric in zip(names,metrics):
            epochs = list(range(1 , 1 + len(metric)))
            plt.plot(epochs, metric, label = name)
    else:
        plt.plot(epochs, metrics)
    
    
    plt.xlabel("Epochs")
    plt.ylabel(metric_name)
    if multi:
        plt.legend()
    
    plt.savefig(image_name + '.png') 



def eval_model(A,X,Y, test_data,rec_k=20,exp_k = 10,alpha = 0.5, N = 5.0):
    
    Ks = [5,10,20,50,100]
    max_K = max(Ks)
    top_k = {}
    precisions = {5:[],10:[],20:[],50:[],100:[]}
    recalls = {5:[],10:[],20:[],50:[],100:[]}
    ndcgs= {5:[], 10:[], 20: [], 50: [], 100:[]}
    top_k_items = []
    for row in test_data:
        user = row[0]
        items = row[1]
        gt_labels = row[2]
        users = [user for _ in items]
        scores = []
        
        user_care = X[user, :]
        idx = np.argpartition(user_care, -exp_k)
        idx = idx[-exp_k:]
        
        for index in list(zip(users,items)):
            u,i = index
            tmp = X[u, idx].dot(Y[i, idx].T) / (exp_k * N)
            score = tmp * alpha + (1 - alpha) * A[u, i]
            scores.append(score)
        
        scores = np.array(scores)
        triples = [(item,ground,score) for item,ground,score in list(zip(items,gt_labels,scores))]
        triples.sort(key = lambda x: x[2],reverse= True)
        final_triples = triples[:max_K]
        
        top_k_items.append([item for item,_,_ in final_triples])
        top_k[user] = final_triples
    
    for row in test_data:
        user = row[0]
        items = row[1]
        triple = top_k[user]
        for top in Ks:
            curr_triple = triple[:top]
            items_recommended,gt_k,score_k = list(zip(*curr_triple))
            items_relevant = [item for i,(item,score,label) in enumerate(zip(items_recommended, score_k,gt_k)) if label == 1]

            common = set(items) & set(items_relevant)
            common_size = len(common)
            recalls[top].append(common_size / len(items))
            precisions[top].append(common_size / min([top, len(items)]))
            ndcg = ndcg_score([gt_labels], [scores], k=top)
            ndcgs[top].append(ndcg)

    returns = ()
    for top in Ks:
        recall = np.mean(recalls[top])
        precision = np.mean(precisions[top])
        
        if precision + recall != 0:
            f1 = 2 * recall * precision / (precision + recall)
        
        else:
            f1 = 0.0
        
        precision = round(precision,10)
        recall = round(recall,10)
        f1 = round(f1,10)
        ndcg = round(np.mean(ndcgs[top]),10)
        if top == rec_k:
            returns = (ndcg, recall, precision, f1)
        print("\t\t", 'NDCG@',top,":",ndcg,end = '\t\t')
        print("Precision@",top,":",precision,'Recall@',top,":", recall," F1@",top,":",f1)
        
    return returns

def training(A, X, Y, r, r_, lambda_x, lambda_y, lambda_u, lambda_h, lambda_v, T, alpha,args,test_data,resume_train = False):
    m = X.shape[0]
    p = X.shape[1]
    n = Y.shape[0]
    if resume_train:
        with open(args.model_path, 'rb') as handle:
            weights = pickle.load(handle)
            U1, U2, V, H1, H2 = weights[0],weights[1],weights[2],weights[3],weights[4]
            
    else:
        U1 = np.random.rand(m, r)
        U2 = np.random.rand(n, r)
        V = np.random.rand(p, r)
        H1 = np.random.rand(m, r_)
        H2 = np.random.rand(n, r_)
    
    
    t = 0
    top_k = int(args.rec_k)
    print("Initial metrics:")
    eval_model(A,X,Y, test_data,top_k,args.exp_k, args.alpha)
    ndcgs,precisions, recalls,f1s = [],[],[],[]
    losses = {'1':[],'2':[],'3':[],'reg':[],'all':[]}
    for t in tqdm(range(1, T + 1)):
        _U1 = U1
        _U2 = U2
        _V = V
        _H1 = H1
        _H2 = H2
        E = 0
        err1 = 0
        err2 = 0 
        err3 = 0
        row1, col1 = np.where(A > 0)
        row2, col2 = np.where(X > 0)
        row3, col3 = np.where(Y > 0)
        for i,j in tqdm(zip(row1,col1)):
                            
            e1_ij = A[i,j] - U1[i, :].dot(U2.T[:, j]) - H1[i, :].dot(H2.T[:, j])
            E += pow(e1_ij, 2)
            for k in range(r):
                _U1[i, k] = U1[i, k] + alpha * (2 * e1_ij * U2[j, k] - 2 * lambda_u * U1[i, k])
                _U2[j, k] = U2[j, k] + alpha * (2 * e1_ij * U1[i, k] - 2 * lambda_u * U2[j, k])
            
            for k in range(r_):
                _H1[i, k] = H1[i, k] + alpha * (2 * e1_ij * H2[j, k] - 2 * lambda_h * H1[i, k])
                _H2[j, k] = H2[j, k] + alpha * (2 * e1_ij * H1[i, k] - 2 * lambda_h * H2[j, k])

        err1 = E
        losses['1'].append(err1)
        for i,j in tqdm(zip(row2,col2)):
                        
                
            e2_ij = X[i, j] - U1[i, :].dot(V.T[:, j])
            E += pow(e2_ij, 2)
            for k in range(r):
                _U1[i, k] = _U1[i, k] +alpha * (2 * e2_ij * V[j, k])
                _V[j, k] = V[j, k] + alpha * (2 * e2_ij * U1[i, k] - 2 * lambda_v * V[j, k])

        err2 = E - err1
        losses['2'].append(err2)
        for i,j in tqdm(zip(row3,col3)):
                
            e3_ij = Y[i, j] - U2[i, :].dot(V.T[:, j])
            E += pow(e3_ij, 2)
            for k in range(r):
                _U2[i, k] = _U2[i, k] + alpha * (2 * e3_ij * V[j, k])
                _V[j, k] = _V[j, k] + alpha * (2 * e3_ij * U2[i, k])
        
        err3 = E - (err1 + err2)
        
        e4 = lambda_u * (np.sum(U1 ** 2) + np.sum(U2 ** 2))
        e5 = lambda_h * (np.sum(H1 ** 2) + np.sum(H2 ** 2))
        e6 = lambda_v * (np.sum(V ** 2))
        
        losses['3'].append(err3)        
        losses['reg'].append(e4 + e5 + e6)
        losses['all'].append(E)
        print("\nLoss after ",t, ' epochs: ', ' Loss 1: ',round(err1,6), ' Loss 2:',round(err2,6)," Loss 3: ",round(err3,6),' Total Loss: ', round(E,6))
        if t % args.print_every == 0 or t == 1 or t == T:
            X_ = U1.dot(V.T)
            Y_ = U2.dot(V.T)
            A_ = U1.dot(U2.T) + H1.dot(H2.T)
            n,re,p,f1 = eval_model(A_,X_,Y_,test_data,top_k,args.exp_k,args.alpha)
            ndcgs.append(n)
            recalls.append(re)
            precisions.append(p)
            f1s.append(f1)
            
        
        E += e4 + e5 + e6

        U1 = _U1
        U2 = _U2
        V = _V
        H1 = _H1
        H2 = _H2
    
    
    plot_metrics(metrics = [losses['1'],losses['all']], metric_name = 'Loss-main',dataset = args.dataset, multi = True,names = ['Loss User-Item','Loss Total'],model_name = 'EFM')
    plot_metrics(metrics = [losses['2'],losses['3']],metric_name = 'Loss-feature',dataset = args.dataset, multi = True, names = ['Loss User-Feature','Loss Item-Feature'],model_name = 'EFM')
    plot_metrics(ndcgs,metric_name = 'NDCG@'+str(top_k), dataset = args.dataset,model_name='EFM')
    plot_metrics(precisions,metric_name = 'Precision@'+str(top_k), dataset = args.dataset,model_name='EFM')
    plot_metrics(recalls,metric_name = 'Recall@'+str(top_k),  dataset = args.dataset, model_name='EFM')
    plot_metrics(f1s,metric_name = 'F1@'+str(top_k),  dataset = args.dataset,model_name='EFM')

    return [U1, U2, V, H1, H2]
              
def main():
    args = args_process()
    print("Args:",args)
    with open(args.data_path, 'rb') as handle:
        rec_dataset = pickle.load(handle)

    print("DATASET:",args.dataset.upper())
    rec_dataset.printDetails()

    user_dim = rec_dataset.user_num
    item_dim = rec_dataset.item_num
    attribute_dim = rec_dataset.feature_num

    
    N = 5 # maximum score
    hidden_dim = int(args.dim)
    
    seed = int(args.seed)

    rec_k = int(args.rec_k)
    exp_k = int(args.exp_k)
    
    random.seed(seed)
    np.random.seed(seed)
    
    user_item_matrix = np.zeros((user_dim, item_dim))
    train_data = rec_dataset.training_data
    # make the A matrix!
    for row in train_data:
            
        user = row[0]
        item = row[1]
        label = row[2]
        user_item_matrix[user,item] = label
    
    
    params = training(user_item_matrix, rec_dataset.user_feature_matrix, rec_dataset.item_feature_matrix, hidden_dim, hidden_dim,args.weight_decay,args.weight_decay,args.weight_decay ,args.weight_decay ,args.weight_decay, int(args.epochs), float(args.lr),args,rec_dataset.test_data,resume_train = args.resume_train)
    name = 'EFM_'+str(args.dataset).upper()
    
    
    name += '.pkl'
    with open("base-models/" + name,'wb') as outp:
        pickle.dump(params, outp, pickle.HIGHEST_PROTOCOL)
        
if __name__ == '__main__':
    main()