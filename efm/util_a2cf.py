import numpy as np
from sklearn.metrics import ndcg_score
import matplotlib.pyplot as plt
import os
from tqdm import tqdm
import sys

def plot_metrics(metrics, metric_name = 'Loss',dataset = "Electronics",adv = False, adv_method = 'fgsm',eps = 0.5, multi = False,names = None,model_name = 'CER-BASE'):
    epochs = list(range(1 , 1 + len(metrics)))
    image_name = model_name+"-plots/" + dataset + "_" + metric_name
    if adv:
        image_name += '_ADV'+"_"+str(adv_method)+"_"+str(eps)
    dirname = os.path.dirname(os.path.abspath(image_name))
    os.makedirs(dirname, exist_ok=True)
    plt.figure(figsize = (12,8))
    plt.title(model_name + "-" + dataset + "-" + metric_name + ' vs Epochs')
    
    if multi:
        
        for name,metric in zip(names,metrics):
            epochs = list(range(1 , 1 + len(metric)))
            plt.plot(epochs, metric, label = name)
    else:
        plt.plot(epochs, metrics)
    
    
    plt.xlabel("Epochs")
    plt.ylabel(metric_name)
    if multi:
        plt.legend()
    
    plt.savefig(image_name + '.png') 
    
def evaluate_model_perspective(
        rec_dict,
        u_i_exp_dict,
        sess,
        predictions,
        q_final,users,items,feature_vec,masks,
        user_feature_matrix,
        item_feature_matrix,
        rec_k,test_num,hidden_dim = 64):
    """
    compute PN, PS and F_NS score for the explanations
    :param rec_dict: {u1: [i1, i2, i3, ...] , u2: [i1, i2, i3, ...]}
    :param u_i_exp_dict: {(u, i): [f1, f2, ...], ...}
    :param sess: the trained session for the recommendation model
    :param predictions: the tensor for generating the top K
    :param user_feature_matrix: |u| x |p| matrix, the attention on each feature p for each user u
    :param item_feature_matrix: |i| x |p| matrix, the quality on each feature p for each item i
    :param rec_k: the length of the recommendation list, only generated explanations for the items on the list
    :return: the mean of the PN, PS and FNS scores
    """
    pn_count = 0
    ps_count = 0
    for u_i, fs in tqdm(list(u_i_exp_dict.items())[:]):
        user = u_i[0]
        target_item = u_i[1]
        if len(fs) == 0:
            continue #no explanation for this pair
        features = set(fs)
#         print("FEATURES:",features)
        curr_items = rec_dict[user]
        
        target_index = curr_items.index(target_item)
        # compute PN
        cf_items_features = []
        
        for item in curr_items:
            item_ori_feature = np.array(item_feature_matrix[item])
#             item_cf_feature = np.array([0 if s in features else item_ori_feature[s]
#                                         for s in range(len(item_ori_feature))], dtype='float32')
            item_cf_feature = np.array([0 if s in features else 1 for s in range(len(item_ori_feature))], dtype='float32')
            cf_items_features.append(item_cf_feature)
#             masks_.append(np.array([0 if s in features else 1 for s in range(len(item_ori_feature))], dtype='float32'))
        
        masks_ = []
        for s in range(item_feature_matrix.shape[1]):
            if s in features:
                masks_.append(np.zeros(shape = hidden_dim))
            else:
                masks_.append(np.ones(shape = hidden_dim))
        
        masks_ = np.array(masks_)
        attention, cf_ranking_scores = sess.run([q_final,predictions], feed_dict = {users:np.array([user for i in range(len(curr_items))]).reshape((-1,1)), items: np.array(curr_items).reshape((-1,1)), feature_vec: np.array(cf_items_features), masks:masks_ })
        
#         print("PN: With raw counterfactual:",cfs)
#         print("PN Projected item feature matrix WEIGHTS:",(attention))
        
        cf_score_list = cf_ranking_scores.ravel()
#         print("Original List for this user:",rec_dict[user])
        
        
        
        sorted_index = np.argsort(cf_score_list)[::-1]
#         print("SORT INDEX:",sorted_index)
#         print("SCORE LIST for PN:",list(zip(np.array(curr_items)[sorted_index],np.array(cf_score_list)[sorted_index])))
        
        cf_rank = np.argwhere(sorted_index == target_index)[0, 0]  # the updated ranking of the current item
#         print("PN: WHERE FOUND target item:",cf_rank)
        
        if cf_rank > rec_k - 1:
            pn_count += 1
        # compute NS
        cf_items_features = []
        # masks_ = []
        for item in curr_items:
            item_ori_feature = np.array(item_feature_matrix[item])
#             item_cf_feature = np.array([item_ori_feature[s] if s in features else 0
#                                         for s in range(len(item_ori_feature))], dtype='float32')
            item_cf_feature = np.array([1 if s in features else 0
                                        for s in range(len(item_ori_feature))], dtype='float32')
            cf_items_features.append(item_cf_feature)
            # masks_.append(np.array([1 if s in features else 0 for s in range(len(item_ori_feature))], dtype='float32').tolist())
        
        masks_ = []
        for s in range(item_feature_matrix.shape[1]):
            if s not in features:
                masks_.append(np.zeros(shape = hidden_dim))
            else:
                masks_.append(np.ones(shape = hidden_dim))
        
        masks_ = np.array(masks_)                            
        
        attention, cf_ranking_scores = sess.run([q_final,predictions], feed_dict = {users:np.array([user for i in range(len(curr_items))]).reshape((-1,1)), items: np.array(curr_items).reshape((-1,1)), feature_vec: np.array(cf_items_features),masks: masks_})
        
#         print("PS With raw counterfactual:",cfs)
#         print("PS Projected item feature matrix:",(attention))
        
        cf_score_list = cf_ranking_scores.ravel()

        sorted_index = np.argsort(cf_score_list)[::-1]
        
#         print("SCORE LIST for PS:",list(zip(np.array(curr_items)[sorted_index],np.array(cf_score_list)[sorted_index])))
        cf_rank = np.argwhere(sorted_index == target_index)[0, 0]  # the updated ranking of the current item
#         print("PS: WHERE FOUND target item:",cf_rank)
        if cf_rank < rec_k:
            ps_count += 1
    if len(u_i_exp_dict) != 0:
        pn = pn_count / len(u_i_exp_dict)
        ps = ps_count / len(u_i_exp_dict)
        if (pn + ps) != 0:
            fns = (2 * pn * ps) / (pn + ps)
        else:
            fns = 0
    else:
        pn = 0
        ps = 0
        fns = 0
    return pn, ps, fns
    
    
    
def evaluate_user_perspective(user_perspective_data, u_i_expl_dict):
    pres = []
    recs = []
    f1s = []
    print("Number of explainable samples:",len(user_perspective_data))
    for u_i, gt_features in user_perspective_data.items():
        if u_i in u_i_expl_dict:
            TP = 0
            pre_features = u_i_expl_dict[u_i]
            print('f: ground: ', gt_features, ' predicted: ',pre_features)
            for feature in pre_features:
                if feature in gt_features:
                    TP += 1
            pre = TP / max(len(pre_features),1)
            rec = TP / max(len(gt_features),1)
            if (pre + rec) != 0:
                f1 = (2 * pre * rec) / (pre + rec)
            else:
                f1 = 0
            pres.append(pre)
            recs.append(rec)
            f1s.append(f1)
    ave_pre = np.mean(pres)
    ave_rec = np.mean(recs)
    ave_f1 = np.mean(f1s)
    return ave_pre, ave_rec, ave_f1  
    
    
def find_validation_data(dataset,test_size = 5):
    val_dataset = []
    for user in range(len(dataset)):
        items = dataset[user][1][:].tolist() # after all test items
        neg_items = items[test_size + 1:]
        for i in range(test_size + 1):
            val_dataset.append([user,items[i],items[i]] + neg_items) # set candidate item same as positive item
     
    return val_dataset

