from __future__ import print_function
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import random
import numpy as np
from sklearn.metrics import ndcg_score
from sklearn.preprocessing import normalize
import argparse
import sys
import matplotlib.pyplot as plt
from tqdm import tqdm
from pathlib import Path
import pickle
sys.path.append("../../data")
from text_processed_dataset import TextProcessedDataset
from yelp_processed_dataset import YelpProcessedDataset
os.environ['CUDA_VISIBLE_DEVICES'] = '2'
def args_process():    # Parse argument
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", dest="dataset", type=str, default="electronics",help = 'Name of the dataset')
    parser.add_argument("--adv", dest="adv", type=bool, default = False, help="whether to do adversarial or not")
    parser.add_argument("--adv_method", dest="adv_method", type=str, default = 'FGSM', help="whether to do which adversarial method? Random or FGSM")
    parser.add_argument("--neg_size", type = int, default = 2, help = 'Negative samples per one positive sample')
    parser.add_argument("--dim", type = int, default = 64, help = 'Hidden Embedding dimensions')
    parser.add_argument("--eps", dest="eps", type=float, default=0.5, help="Error proportion for adversarial training")
    parser.add_argument("--seed", dest="seed", type=int, default=999, help="seed for training")
    parser.add_argument("--test_num", dest="test_num", type=int, default=-1, help="number of users for testing")
    parser.add_argument("--verbose", type = int, default = 0, help = 'Print extra info')
    parser.add_argument("--data_path",dest = 'data_path', type=str, default ='../data/final-obj/Electronics_dataset_obj.pickle',help = "Path to the dataset for loading")
    parser.add_argument("--rec_k", dest="rec_k", type=int, default=20, help="length of rec list")
    parser.add_argument("--last_exp_k", dest="last_exp_k", type=int, default=5, help="length of final expl list")
    parser.add_argument("--first_exp_k", dest="first_exp_k", type=int, default=5, help="length of first expl list from X prime")
    parser.add_argument("--model_explain",dest = "model_explain",type = bool,default = False, help = 'Model evaluation for expl?')
    parser.add_argument("--alpha", dest="alpha", type=float, default=0.5, help="ratio for the personalized recommendation")
    parser.add_argument("--model_path", dest="model_path", type=str, default="./logs/",help = 'path to base recommender model')
    parser.add_argument("--save_path", dest="save_path", type=str, default="expl-models/",help = 'path to save expl model')
    args = parser.parse_args()
    return args
def evaluate_user_perspective(user_perspective_data, u_i_expl_dict):
    pres = []
    recs = []
    f1s = []
    for u_i, gt_features in user_perspective_data.items():
        if u_i in u_i_expl_dict:
            TP = 0
            pre_features = u_i_expl_dict[u_i]
            # print('f: ground: ', gt_features, ' predicted: ',pre_features)
            for feature in pre_features:
                if feature in gt_features:
                    TP += 1
            pre = TP / max(len(pre_features),1)
            rec = TP / max(len(gt_features),1)
            if (pre + rec) != 0:
                f1 = (2 * pre * rec) / (pre + rec)
            else:
                f1 = 0
            pres.append(pre)
            recs.append(rec)
            f1s.append(f1)
    ave_pre = np.mean(pres)
    ave_rec = np.mean(recs)
    ave_f1 = np.mean(f1s)
    return ave_pre, ave_rec, ave_f1 

def evaluate_model_perspective(rec_dict,u_i_exp_dict,test_data,A,X,Y,rec_k,first_exp_k,last_exp_k, N = 5,alpha = 0.85):
    
    pn_count = 0
    ps_count = 0
    for u_i, fs in tqdm(list(u_i_exp_dict.items())[:]):
        user = u_i[0]
        target_item = u_i[1]
        if len(fs) == 0:
            continue #no explanation for this pair
        features = set(fs)
        curr_items = rec_dict[user]
        
        target_index = curr_items.index(target_item)
        
        # compute PN
        cf_items_features = []
        
        for item in curr_items:
            item_ori_feature = np.array(Y[item])
            item_cf_feature = np.array([0 if s in features else item_ori_feature[s]
                                        for s in range(len(item_ori_feature))], dtype='float32')
            cf_items_features.append(item_cf_feature)
        
        user_care = X[user, :]
        idx = np.argpartition(user_care, -first_exp_k)
        idx = idx[-first_exp_k:]
        
        
        
        users = [user for _ in curr_items]
        scores = []
        cf_items_features = np.array(cf_items_features)
        for i,index in enumerate(list(zip(users,curr_items))):
            u,_ = index
            tmp = X[u, idx].dot(cf_items_features[i, idx].T) / (first_exp_k * N)
            score = tmp * alpha + (1 - alpha) * A[u, curr_items[i]]
            scores.append(score)
        
        
        cf_ranking_scores = np.array(scores)
        cf_score_list = cf_ranking_scores
        sorted_index = np.argsort(cf_score_list)[::-1]
        cf_rank = np.argwhere(sorted_index == target_index)[0, 0]  # the updated ranking of the current item
        
        if cf_rank > rec_k - 1:
            pn_count += 1
        
        
        
        # compute NS
        cf_items_features = []
        for item in curr_items:
            item_ori_feature = np.array(Y[item])
            item_cf_feature = np.array([item_ori_feature[s] if s in features else 0
                                        for s in range(len(item_ori_feature))], dtype='float32')

            cf_items_features.append(item_cf_feature)
        
        cf_items_features = np.array(cf_items_features)
        user_care = X[user, :]
        idx = np.argpartition(user_care, -first_exp_k)
        idx = idx[-first_exp_k:]
        
        users = [user for _ in curr_items]
        
        scores = []
        for i,index in enumerate(list(zip(users,curr_items))):
            u,_ = index
            tmp = X[u, idx].dot(cf_items_features[i, idx].T) / (first_exp_k * N)
            score = tmp * alpha + (1 - alpha) * A[u, curr_items[i]]
            scores.append(score)
        
        cf_ranking_scores = np.array(scores)
        cf_score_list = cf_ranking_scores     
        sorted_index = np.argsort(cf_score_list)[::-1]
        cf_rank = np.argwhere(sorted_index == target_index)[0, 0]  # the updated ranking of the current item
        
        if cf_rank < rec_k:
            ps_count += 1
    if len(u_i_exp_dict) != 0:
        pn = pn_count / len(u_i_exp_dict)
        ps = ps_count / len(u_i_exp_dict)
        if (pn + ps) != 0:
            fns = (2 * pn * ps) / (pn + ps)
        else:
            fns = 0
    else:
        pn = 0
        ps = 0
        fns = 0
    return pn, ps, fns


def expl_model(A,X,Y, test_data, test_num, user_item_feature_dict,rec_k=5,first_exp_k = 50,last_exp_k = 5,alpha = 0.5, N = 5.0):
    
    
    rec_dict = {}
    u_i_expl_dict = {}
    correct_rec_dict = {}
    
    
    
    for row in test_data:
        user = row[0]
        items = row[1]
        labels = row[2]
        users = [user for _ in items]
        scores = []
        correct_rec_dict[user] = []
        user_care = X[user, :]
        idx = np.argpartition(user_care, -first_exp_k)
        idx = idx[-first_exp_k:]
        
        for index in list(zip(users,items)):
            u,i = index
            tmp = X[u, idx].dot(Y[i, idx].T) / (first_exp_k * N)
            score = tmp * alpha + (1 - alpha) * A[u, i]
            scores.append(score)
        
        scores = np.array(scores)
        sort_index = sorted(range(len(scores)), key=lambda x: scores[x], reverse=True)
        sorted_items = [items[j] for j in sort_index]
        rec_dict[user] = sorted_items
        if user < 5:
            print("recommendation for user:",user," :",list(zip(sorted_items,labels[sort_index], scores[sort_index]))[:rec_k])
        
        for i in range(rec_k):  # find the correct items and add to the user side test data
            if labels[sort_index[i]] == 1:
                correct_rec_dict[user].append(items[sort_index[i]])
        
    
    if test_num == -1:
        test_num = len(list(rec_dict.items()))
    
    
    print("Finished recommendations!")
    user_perspective_test_data = {}  # {(u, i):f, (u, i): f]}
    for batch_user, batch_items in correct_rec_dict.items():
        for item in batch_items:
            feature = user_item_feature_dict[(batch_user, item)]
            user_perspective_test_data[(batch_user, item)] = feature
    
    
    
    print("Beginning for generating all the explanations!\n")
    for i, (user,rec_items) in tqdm(enumerate(list(rec_dict.items())[:test_num])):
        for item in rec_items[:rec_k]:
            
            user_care = X[user, :]
            idx = np.argpartition(user_care, -first_exp_k)
            idx = idx[-first_exp_k:]
            
            item_care_idx = Y[item, idx]
            
            explanation_features_all = list(zip(idx, item_care_idx))
            explanation_features_all.sort(key = lambda x: x[1], reverse = True)
            explanation_features, _ = zip(*explanation_features_all)
            
            
            print('explanation for user %d and item %d' % (user, item), end = ',')
            print("features:",explanation_features[:last_exp_k])
            u_i_expl_dict[(user,item)] = explanation_features[:last_exp_k]
        print("="*50)
        # sys.exit()
        
    
    print("Generated all the explanations!")
    print("Time to evaluate now")
    
    ave_pre, ave_rec, ave_f1 = evaluate_user_perspective(user_perspective_test_data,u_i_expl_dict)
    print('user\'s perspective:')
    print('ave pre: ', round(ave_pre,6), '  ave rec: ', round(ave_rec,6), '  ave f1: ', round(ave_f1,6))
    # ave_pn, ave_ps, ave_fns = evaluate_model_perspective(rec_dict,u_i_expl_dict,test_data,A,X,Y,rec_k,first_exp_k,last_exp_k,N = N,alpha = alpha)
    # print('model\'s perspective:')
    # print('ave PN: ', round(ave_pn,6), '  ave PS: ', round(ave_ps,6), '  ave F_{NS}: ', round(ave_fns,6))  
    return (rec_dict, u_i_expl_dict)
        

def model_explain(rec_dict,u_i_expl_dict,test_data,A,X,Y,rec_k,first_exp_k,last_exp_k,N,alpha):
    ave_pn, ave_ps, ave_fns = evaluate_model_perspective(rec_dict,u_i_expl_dict,test_data,A,X,Y,rec_k,first_exp_k,last_exp_k,N = N,alpha = alpha)
    print('model\'s perspective:')
    print('ave PN: ', round(ave_pn,6), '  ave PS: ', round(ave_ps,6), '  ave F_{NS}: ', round(ave_fns,6))  
    
    
def main():
    args = args_process()
    print("Args:",args)
    with open(args.data_path, 'rb') as handle:
        rec_dataset = pickle.load(handle)

    print("DATASET:",args.dataset.upper())
    rec_dataset.printDetails()

    user_dim = rec_dataset.user_num
    item_dim = rec_dataset.item_num
    attribute_dim = rec_dataset.feature_num

    pos_size = 6 # change based on what test data you use depending on number of positive samples per user
    N = 5 # maximum score
    hidden_dim = int(args.dim)
    
    seed = int(args.seed)
    adv = bool(args.adv)
    adv_method = str(args.adv_method)
    eps = float(args.eps)
    rec_k = int(args.rec_k)
    first_exp_k = int(args.first_exp_k)
    last_exp_k = int(args.last_exp_k)
    test_data = rec_dataset.test_data
    random.seed(seed)
    np.random.seed(seed)
    with open(args.model_path, 'rb') as handle:
        weights = pickle.load(handle)
    
    
    if adv:
        U1,U2,V,H1,H2,U1noise, U2noise, Vnoise = weights[0],weights[1], weights[2], weights[3], weights[4], weights[5],weights[6], weights[7]
    
    else:
        U1,U2,V,H1,H2 = weights[0],weights[1], weights[2], weights[3], weights[4]
        U1noise = np.zeros_like(U1)
        U2noise = np.zeros_like(U2)
        Vnoise = np.zeros_like(V)
    
    
    X_ = (U1 + U1noise).dot(V.T + Vnoise.T)
    Y_ = (U2 + U2noise).dot(V.T + Vnoise.T)
    A_ = (U1 + U1noise).dot(U2.T + U2noise.T) + H1.dot(H2.T)
    
    if args.model_explain:
        dir_name = str(args.save_path) + str(args.dataset)
        if adv:
            dir_name += "_" + str(adv_method.upper()) + "_" + str(eps)

        name = dir_name + "/explanation_obj.pickle"
        # Path(dir_name).mkdir(parents=True, exist_ok=True)

        with open(name, 'rb') as outp:
            outputs = pickle.load(outp)
            rec_dict,u_i_expl_dict = outputs[0],outputs[1] 
        model_explain(rec_dict,u_i_expl_dict, test_data, A_,X_,Y_, rec_k,first_exp_k,last_exp_k,N=5,alpha = args.alpha)
        sys.exit()
    
    
    user_item_feature_dict = {}  # {(u, i): f, (u, i): f]
    # Get the whole user-item-feature information
    for row in rec_dataset.sentiment_data:
        user = row[0]
        item = row[1]
        user_item_feature_dict[(user, item)] = []
        for fos in row[2:]:
            feature = fos[0]
            user_item_feature_dict[(user, item)].append(feature)
    
    outputs = expl_model(A_,X_,Y_, test_data, args.test_num,user_item_feature_dict,rec_k=rec_k,first_exp_k = first_exp_k,last_exp_k = last_exp_k,alpha = args.alpha, N = 5.0)
    dir_name = str(args.save_path) + str(args.dataset)
    if adv:
        dir_name += "_" + str(adv_method.upper()) + "_" + str(eps)

    name = dir_name + "/explanation_obj.pickle"
    Path(dir_name).mkdir(parents=True, exist_ok=True)

    with open(name, 'wb') as outp:
        pickle.dump(outputs, outp, pickle.HIGHEST_PROTOCOL)

    

if __name__ == '__main__':
    main()