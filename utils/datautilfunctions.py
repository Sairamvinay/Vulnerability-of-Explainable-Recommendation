import numpy as np
import math
import matplotlib.pyplot as plt
import argparse
import pandas as pd
import gzip
import json
import warnings
warnings.filterwarnings("ignore")
np.random.seed(999)

# Credits to https://github.com/chrisjtan/counter/
def parse(path):
    g = gzip.open(path, 'r')
    for l in g:
        yield json.loads(l)

def getDFjson(path):
    i = 0
    df = {}
    for d in parse(path):
        df[i] = d
        i += 1
    return pd.DataFrame.from_dict(df, orient='index')

def getJsonFile(path):
    '''
    This function loads the data from json file 
    to data frame.
    
    '''
    lines = []
    with open(path) as f:
        for i,line in enumerate(f):
            
            lines.append(json.loads(line))
    
    return lines

def getDFcsv(path):
    df = pd.read_csv(path)
    return df

def reportDetails(sentiment_data,user_dict, item_dict,features,opinions):
    M,N = len(user_dict),len(item_dict)
    print('processed reviews count: ', len(sentiment_data))
    print("processed users count: ", M)
    print('processed items count: ', N)
    print("processed features count: ", len(features)) 
    print("processed opinions count: ", len(opinions))

def arg_parser_preprocessing():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", dest="dataset", type=str, default="clothing")
    parser.add_argument("--sentires_dir", dest="sentires_dir", type=str, default="../sentiment_data/clothing", 
                        help="path to pre-extracted sentires data")
    parser.add_argument("--review_dir", dest="review_dir", type=str, default="../data/cleaned-data/cleaned-clothing.csv", 
                        help="path to original review data")
    parser.add_argument("--user-col",dest = 'user_col', type = str, default = 'reviewerID',help = 'User ID column name in review dataset')
    parser.add_argument("--item-col",dest = 'item_col', type = str, default = 'asin',help = 'Item ID column name in review dataset')
    parser.add_argument("--time-col",dest = 'time_col', type = str, default = 'unixReviewTime',help = 'Timestamp column name in review dataset')
    parser.add_argument("--debug",dest = 'debug', type = str, default = 0,help = 'Print extra information while filtering datasets')
    
    parser.add_argument("--user_thresh", dest="user_thresh", type=int, default=20, 
    help="remove users with reviews less than this threshold")
    
    parser.add_argument("--item_thresh", dest="item_thresh", type=int, default=0, 
    help="remove items with reviews less than this threshold")
    
    parser.add_argument("--feature_thresh", dest="feature_thresh", type=int, default=5, 
    help="remove the features mentioned less than this threshold")
    
    parser.add_argument("--sample_ratio", dest="sample_ratio", type=int, default=2, 
                        help="the (negative: positive sample) ratio for training BPR loss")
    parser.add_argument("--test_length", dest="test_length", type=int, default=5, 
    help="the number of test items")
    parser.add_argument("--val_length", dest="val_length", type=int, default=1, 
    help="the number of val items")
    parser.add_argument("--neg_length", dest="neg_length", type=int, default=100, help="# of negative samples in evaluation")
    parser.add_argument("--save_path", dest="save_path", type=str, default="./final-obj/", 
    help="The path to save the preprocessed dataset object")
    return parser.parse_args()


def sentiment_data_filtering(sentiment_data, user_thresh, item_thresh, feature_thresh,debug):
    """
    filter the sentiment data, remove the users and items with less review number less than "user_thresh" and "item_thresh" respectively and remove the features
    mentioned less than "feature_thresh" or don't contain letters.
    :param sentiment_data: [userID, itemID, [fos triplet 1], [fos triplet 2], ...]
    :param user_thresh: the threshold for user reviews
    :param feature_thresh: the threshold features
    :return: the filtered sentiment data
    """
    print('======================= filtering sentiment data =======================')
    sentiment_data = np.array(sentiment_data)
    last_length = len(sentiment_data)
    un_change_count = 0  # iteratively filtering users and features, if the data stay unchanged twice, stop
    user_dict, item_dict = get_user_item_dict(sentiment_data)
    features,opinions = get_feature_list(sentiment_data)
    print("original reviews count: ", len(sentiment_data))
    print("original users count: ", len(user_dict))
    print("original items count: ", len(item_dict))
    print("original features count: ", len(features))
    print("original opinions count: ", len(opinions))
    epoch = 1
    while True:
        # feature filtering
        feature_count_dict = {}
        for row in sentiment_data:
            for fos in row[2:]:
                feature = fos[0]
                if feature not in feature_count_dict:
                    feature_count_dict[feature] = 1
                else:
                    feature_count_dict[feature] += 1
        valid_features = set()
        for key, value in feature_count_dict.items():
            if check_string(key) and value > feature_thresh:
                valid_features.add(key)
        
        sentiment_data = feature_filtering(sentiment_data, valid_features)
        length = len(sentiment_data)
        if debug:
            print("Epoch ",epoch)
            print("Removing all pairs without f,o,s data and filtering features with feature threshold count: ", feature_thresh ,"; Review count: ", length)
        
        if length != last_length:
            last_length = length
            un_change_count = 0
        else:
            un_change_count += 1
            if un_change_count == 2:
                break
        # user filtering
        user_dict, item_dict = get_user_item_dict(sentiment_data)
        valid_user = set()  # the valid users
        for key, value in user_dict.items():
            if len(value) > (user_thresh - 1):
                valid_user.add(key)
        
        valid_item = set() # the valid items
        for key, value in item_dict.items():
            if len(value) > (item_thresh - 1):
                valid_item.add(key)
        
        if debug:
            print("Users with atleast ", user_thresh," reviews: ",len(valid_user))
            print("Items with atleast ",item_thresh, " reviews: ",len(valid_item))
        
        sentiment_data = [x for x in sentiment_data if (x[0] in valid_user and x[1] in valid_item)]  # remove user and item with small interactions
        # sentiment_data = [x for x in sentiment_data if x[1] in valid_item]  # remove item with small interactions
        user_dict, item_dict = get_user_item_dict(sentiment_data)
        features,opinions = get_feature_list(sentiment_data)
        
        if debug:
            reportDetails(sentiment_data,user_dict, item_dict,features,opinions)
        
        epoch += 1
        
        length = len(sentiment_data)
        if length != last_length:
            last_length = length
            un_change_count = 0
        else:
            un_change_count += 1
            if un_change_count == 2:
                break
    user_dict, item_dict = get_user_item_dict(sentiment_data)
    features,opinions = get_feature_list(sentiment_data)
    M,N = len(user_dict),len(item_dict)
    density = len(sentiment_data) / (M * N)
    
    print('final valid reviews count: ', len(sentiment_data))
    print("final valid users count: ", M)
    print('final valid items count: ', N)
    print("final valid features count: ", len(features))
    print("final valid opinions count: ", len(opinions))
    print("final Density of the dataset: ", round(density * 100.00,6),'%')
    sentiment_data = np.array(sentiment_data)
    return sentiment_data


def get_feature_list(sentiment_data):
    """
    from user sentiment data, get all the features [F1, F2, ..., Fk] mentioned in the reviews
    :param sentiment_data: [user, item, [feature1, opinion1, sentiment1], [feature2, opinion2, sentiment2] ...]
    :return: feature set F
    """
    feature_list = []
    opinion_list = []
    for row in sentiment_data:
        for fos in row[2:]:
            feature = fos[0]
            opinion = fos[1]
            if feature not in feature_list:
                feature_list.append(feature)
            
            if opinion not in opinion_list:
                opinion_list.append(opinion)
    
    
    feature_list = np.array(feature_list)
    opinion_list = np.array(opinion_list)
    return feature_list,opinion_list


def get_user_attention_matrix(sentiment_data, user_num, feature_list, max_range=5):
    """
    build user attention matrix
    :param sentiment_data: [user, item, [feature1, opinion1, sentiment1], [feature2, opinion2, sentiment2] ...]
    :param user_num: number of users
    :param feature_list: [F1, F2, ..., Fk]
    :param max_range: normalize the attention value to [1, max_range]
    :return: the user attention matrix, Xij is user i's attention on feature j
    """
    user_counting_matrix = np.zeros((user_num, len(feature_list)))  # tij = x if user i mention feature j x times
    for row in sentiment_data:
        user = row[0]
        for fos in row[2:]:
            feature = fos[0]
            user_counting_matrix[user, feature] += 1
    user_attention_matrix = np.zeros((user_num, len(feature_list)))  # xij = [1-N], normalized attention matrix
    for i in range(len(user_counting_matrix)):
        for j in range(len(user_counting_matrix[i])):
            if user_counting_matrix[i, j] == 0:
                norm_v = 0  # if nor mentioned: 0
            else:
                norm_v = 1 + (max_range - 1) * ((2 / (1 + np.exp(-user_counting_matrix[i, j]))) - 1)  # norm score
            user_attention_matrix[i, j] = norm_v
    user_attention_matrix = np.array(user_attention_matrix, dtype='float32')
    return user_attention_matrix


def get_item_quality_matrix(sentiment_data, item_num, feature_list, max_range=5):
    """
    build item quality matrix
    :param sentiment_data: [user, item, [feature1, opinion1, sentiment1], [feature2, opinion2, sentiment2] ...]
    :param item_num: number of items
    :param feature_list: [F1, F2, ..., Fk]
    :param max_range: normalize the quality value to [1, max_range]
    :return: the item quality matrix, Yij is item i's quality on feature j
    """
    item_counting_matrix = np.zeros((item_num, len(feature_list)))  # kij = x if item i's feature j is mentioned x times
    item_sentiment_matrix = np.zeros((item_num, len(feature_list)))  # sij = x if the overall rating is x (sum up)
    for row in sentiment_data:
        item = row[1]
        for fos in row[2:]:
            feature = fos[0]
            sentiment = fos[2]
            item_counting_matrix[item, feature] += 1
            if sentiment == '+1':
                item_sentiment_matrix[item, feature] += 1
            elif sentiment == '-1':
                item_sentiment_matrix[item, feature] -= 1
            else:
                print("sentiment data error: the sentiment value can only be +1 or -1")
                exit(1)
    item_quality_matrix = np.zeros((item_num, len(feature_list)))
    for i in range(len(item_counting_matrix)):
        for j in range(len(item_counting_matrix[i])):
            if item_counting_matrix[i, j] == 0:
                norm_v = 0  # if not mentioned: 0
            else:
                norm_v = 1 + ((max_range - 1) / (1 + np.exp(-item_sentiment_matrix[i, j])))  # norm score
            item_quality_matrix[i, j] = norm_v
    item_quality_matrix = np.array(item_quality_matrix, dtype='float32')
    return item_quality_matrix


def get_user_item_dict(sentiment_data):
    """
    build user & item dictionary
    :param sentiment_data: [user, item, [feature1, opinion1, sentiment1], [feature2, opinion2, sentiment2] ...]
    :return: user dictionary {u1:[i, i, i...], u2:[i, i, i...]}, similarly, item dictionary
    """
    user_dict = {}
    item_dict = {}
    for row in sentiment_data:
        user = row[0]
        item = row[1]
        if user not in user_dict:
            user_dict[user] = [item]
        else:
            user_dict[user].append(item)
        if item not in item_dict:
            item_dict[item] = [user]
        else:
            item_dict[item].append(user)
    return user_dict, item_dict


def get_user_item_set(sentiment_data):
    """
    get user item set
    :param sentiment_data: [user, item, [feature1, opinion1, sentiment1], [feature2, opinion2, sentiment2] ...]
    :return: user_set = set(u1, u2, ..., um); item_set = (i1, i2, ..., in)
    """
    user_set = set()
    item_set = set()
    for row in sentiment_data:
        user = row[0]
        item = row[1]
        user_set.add(user)
        item_set.add(item)
    return user_set, item_set


def sample_training_pairs(user, training_items, item_set, sample_ratio=10):
    positive_items = set(training_items)
    negative_items = set()
    for item in item_set:
        if item not in positive_items:
            negative_items.add(item)
    neg_length = len(positive_items) * sample_ratio # we take only sampled subset of all negative items per user since # items is a very large number and train set becomes huge and no uniformity per user
    negative_items = np.random.choice(np.array(list(negative_items)), min(neg_length,len(negative_items)), replace=False)
    train_pairs = []
    for p_item in positive_items:
        train_pairs.append([user, p_item, 1])
    for n_item in negative_items:
        train_pairs.append([user, n_item, 0])
    return train_pairs


def check_string(string):
    # if the string contains letters
    string_lowercase = string.lower()
    contains_letters = string_lowercase.islower()
    return contains_letters


def visualization(train_losses, val_losses, path):
    plt.plot(np.arange(len(train_losses)), train_losses, label='training loss')
    plt.plot(np.array(len(val_losses)), val_losses, label='validation loss')
    plt.legend()
    plt.savefig(path)
    plt.clf()


def feature_filtering(sentiment_data, valid_features):
    """
    filter the sentiment data, remove the invalid features
    :param sentiment_data: [userID, itemID, [fos triplet 1], [fos triplet 2], ...]
    :param valid_features: set of valid features
    :return: the filtered sentiment data
    """
    cleaned_sentiment_data = []
    for row in sentiment_data:
        user = row[0]
        item = row[1]
        cleaned_sentiment_data.append([user, item])
        for fos in row[2:]:
            if fos[0] in valid_features:
                cleaned_sentiment_data[-1].append(fos)
        if len(cleaned_sentiment_data[-1]) == 2:
            del cleaned_sentiment_data[-1]
    return np.array(cleaned_sentiment_data)