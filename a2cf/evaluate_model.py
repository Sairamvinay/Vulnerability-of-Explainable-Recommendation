from __future__ import print_function
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
from tensorflow.python import pywrap_tensorflow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import random
import numpy as np
import pickle
from sklearn.metrics import ndcg_score
from sklearn import metrics
import math
import sys
from tqdm import tqdm
import argparse
sys.path.append("../../data")
from text_processed_dataset import TextProcessedDataset
from yelp_processed_dataset import YelpProcessedDataset
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
print("TF version:",tf.__version__)


def args_process():    # Parse argument
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", dest="dataset", type=str, default="electronics",help = 'Name of the dataset')
    parser.add_argument("--adv", dest="adv", type=bool, default = False, help="whether to do adversarial or not")
    parser.add_argument("--adv_method", dest="adv_method", type=str, default = 'FGSM', help="whether to do which adversarial method? Random or FGSM")
    parser.add_argument("--dim", type = int, default = 64, help = 'Hidden Embedding dimensions')
    parser.add_argument("--eps", dest="eps", type=float, default=0.5, help="Error proportion for adversarial training")
    parser.add_argument("--seed", dest="seed", type=int, default=999, help="seed for training")
    parser.add_argument("--data_path",dest = 'data_path', type=str, default ='../data/final-obj/Electronics_dataset_obj.pickle',help = "Path to the dataset for loading")
    parser.add_argument("--model_path", dest="model_path", type=str, default="./logs/",help = 'Load the trained model')
    args = parser.parse_args()
    return args
def print_total_parameters(params, adv = False):
    total_parameters = 0 
    for param in params:
        variable = params[param]
        shape = variable.get_shape()
        variable_parameters = 1 
        for dim in shape:
            variable_parameters *= dim.value
        print('%s  dim=%i shape=%s params=%i' % ( 
                    variable.name,
                    len(shape),
                    shape,
                    variable_parameters,
                    ))  
        print("Variable value:",variable.eval())
        total_parameters += variable_parameters
    print('total_parameters = %i' % (total_parameters))

    return

def EstPnQ(params):
    '''
    The NCF approach is too slow when estimating the full matrices, hence we replace that with a quicker multiplication form.
    With well trained user/item/attribute embeddings, we find that there are no obvious differences in the actual recommendation performance. 
    '''
    with tf.device("/CPU:0"):
        P = tf.nn.tanh(tf.matmul(params["user_embedding"] + params['user_noise_embedding'], tf.transpose(params["attribute_embedding"] + params["attribute_noise_embedding"])))
        P = (0.5*N - 0.5)*P + 0.5*N + 0.5
        Q = tf.nn.tanh(tf.matmul(params["item_embedding"] + params["item_noise_embedding"], tf.transpose(params["attribute_embedding"] + params["attribute_noise_embedding"])))
        Q = (0.5*N - 0.5)*Q + 0.5*N + 0.5
        
        assign_op_P = tf.assign(params["P"], P)
        assign_op_Q = tf.assign(params["Q"], Q)
    return assign_op_P, assign_op_Q

def get_prediction(params,user, items,hidden_dim,attribute_dim,epsilon):
    user_embedding = tf.reshape(tf.nn.embedding_lookup(params["user_embedding"], user), [-1, hidden_dim]) + tf.reshape(tf.nn.embedding_lookup(params["user_noise_embedding"], user), [-1, hidden_dim]) 
    item_embedding = tf.reshape(tf.nn.embedding_lookup(params["item_embedding"], items), [-1, hidden_dim]) + tf.reshape(tf.nn.embedding_lookup(params["item_noise_embedding"], items), [-1, hidden_dim]) 
    
    p_query = tf.reshape(tf.nn.embedding_lookup(params["P"], user), [-1, attribute_dim])
    q_candidate = tf.reshape(tf.nn.embedding_lookup(params["Q"], items), [-1, attribute_dim]) #(batch, attr_size)
    
    ita = tf.nn.softmax(tf.multiply(p_query, q_candidate)/epsilon, axis = 1) #(batch, attr_size)
    v_candidate_tilde_2 = tf.matmul(ita, params["attribute_embedding"] + params["attribute_noise_embedding"]) #(batch, hidden)
    f_p_temp = tf.concat([tf.multiply(user_embedding, item_embedding), v_candidate_tilde_2], 1) #(batch, 2*hidden)
    f_p = tf.matmul(f_p_temp, params["p_projection_weight"] + params["p_noise_projection_weight"]) #(batch, 1)
    f_p = tf.sigmoid(f_p)
    return f_p


'''

network parameters
'''


args = args_process()
print("Args:",args)
with open(args.data_path, 'rb') as handle:
    rec_dataset = pickle.load(handle)

print("DATASET:",args.dataset.upper())
rec_dataset.printDetails()

user_dim = rec_dataset.user_num
item_dim = rec_dataset.item_num
attribute_dim = rec_dataset.feature_num

pos_size = 6 # change based on what test data you use depending on number of positive samples per user
N = 5 # maximum score
hidden_dim = int(args.dim)
beta = 8
epsilon = 8
gamma = 0 # so that we remove item similarity conditions #0.7 # 0<gamma<1
seed = int(args.seed)
adv = bool(args.adv)
adv_method = str(args.adv_method)
eps = float(args.eps)
random.seed(seed)
np.random.seed(seed)
tf.set_random_seed(seed)

test_data = rec_dataset.test_data

print("ARGS:",args)

'''
define parameters (weights & biases)
'''
params = {
    #____ embedding weights ____
    "user_embedding": tf.get_variable("user_embedding", shape = [user_dim, hidden_dim], trainable = not bool(args.adv), initializer = tf.glorot_uniform_initializer),
    "item_embedding": tf.get_variable("item_embedding", shape = [item_dim, hidden_dim], trainable = not bool(args.adv), initializer = tf.glorot_uniform_initializer),
    "attribute_embedding": tf.get_variable("helpful_embedding", shape = [attribute_dim, hidden_dim], trainable = not bool(args.adv), initializer = tf.glorot_uniform_initializer),
    #____ noise weights _____
    "user_noise_embedding": tf.get_variable("user_noise_embedding", shape = [user_dim, hidden_dim], trainable = bool(args.adv), initializer = tf.zeros_initializer),
    "item_noise_embedding": tf.get_variable("item_noise_embedding", shape = [item_dim, hidden_dim], trainable = bool(args.adv), initializer = tf.zeros_initializer),
    "attribute_noise_embedding": tf.get_variable("helpful_noise_embedding", shape = [attribute_dim, hidden_dim], trainable = bool(args.adv), initializer = tf.zeros_initializer),
    "p_noise_projection_weight": tf.get_variable("p_noise_projection_weight", shape = [2*hidden_dim, 1], trainable = bool(args.adv), initializer = tf.zeros_initializer),
    
    #____ MF weights ____    
    "s_projection_weight": tf.get_variable("s_projection_weight", shape = [2*hidden_dim, 1], trainable = not bool(args.adv), initializer = tf.glorot_uniform_initializer),
    "p_projection_weight": tf.get_variable("p_projection_weight", shape = [2*hidden_dim, 1], trainable = not bool(args.adv), initializer = tf.glorot_uniform_initializer)}

with tf.device("/CPU:0"):    
    params["P"] = tf.get_variable("P", trainable = False, initializer = tf.zeros([user_dim, attribute_dim]), caching_device = "/CPU:0")
    params["Q"] = tf.get_variable("Q", trainable = False, initializer = tf.zeros([item_dim, attribute_dim]), caching_device = "/CPU:0")
    

users = tf.placeholder("int32", [None, 1]) #(batch, 1)
items = tf.placeholder("int32", [None, 1]) #(batch, 1)
new_P, new_Q = EstPnQ(params)
predictions = get_prediction(params, users, items,hidden_dim,attribute_dim, epsilon)


with tf.Session() as sess:
    reader = pywrap_tensorflow.NewCheckpointReader(args.model_path)
    weights = ['user_embedding','item_embedding','helpful_embedding','p_projection_weight', 's_projection_weight','user_noise_embedding','item_noise_embedding','helpful_noise_embedding','p_noise_projection_weight']
    _params = [params['user_embedding'],params['item_embedding'],params['attribute_embedding'],params['p_projection_weight'],params['s_projection_weight'],params['user_noise_embedding'],params['item_noise_embedding'],params['attribute_noise_embedding'],params['p_noise_projection_weight']]
    ops = []
    for param,weight in zip(_params, weights):
        t = reader.get_tensor(weight)
        ops.append(param.assign(t))


    sess.run(ops)
    sess.run([new_P,new_Q])
    print("-> Total model parameters loaded from vanilla model")
    print_total_parameters(params)
    
    KList = [5,10,20,50,100]
    K = max(KList)
    
    top_K = {}
    for i,row in (enumerate(test_data)):
        
        batch_user = row[0]
        batch_items = row[1]
        ground_truth = row[2]
        batch_users = [batch_user for _ in range(len(batch_items))]
        prediction_per_user = sess.run(predictions, feed_dict = {users:np.reshape(batch_users,(-1,1)), items:np.reshape(batch_items,(-1,1))})
        prediction_per_user = prediction_per_user.ravel()
        
        # get the top K items recommended
        # then we find the ground truths for these items along with its recommended scores
        triples = [(item,ground,score) for item,ground,score in list(zip(batch_items,ground_truth,prediction_per_user))]
        triples.sort(key = lambda x: x[2],reverse= True)
        
        final_triples = triples[:K]
        top_K[batch_user] = final_triples
        if i < 5:
            print("User :",batch_user, ' items recommended: ',triples[:10])
            print("True items:",list(zip(batch_items[:20],ground_truth[:20])))
        
   
    for k in KList:
        ndcgs = []
        precisions = []
        recalls = []
        for i, row in tqdm(enumerate(test_data)):
            triple = top_K[row[0]][:k]
            batch_items = row[1]
            items_recommended,gt_k,score_k = zip(*triple)
            items_relevant = [item for item,score,label in zip(items_recommended, score_k,gt_k) if (score >= 0.5 and label == 1) or (score < 0.5 and label == 0)]
            # items_relevant = [item for item,score,label in zip(items_recommended, score_k,gt_k) if label == 1]
            ndcgs.append(ndcg_score([gt_k],[score_k],k = k))
            common = set(batch_items) & set(items_relevant)
            common_size = len(common)
            recalls.append(common_size / len(batch_items))
            precisions.append(common_size / min([k, len(batch_items)]))
        
        av_n = round(np.mean(ndcgs),6)
        av_p = round(np.mean(precisions),6)
        av_r = round(np.mean(recalls),6)
        if av_p + av_r == 0:
            av_f = 0
        
        else:
            av_f = 2 * av_p * av_r / (av_p + av_r)
        print("NDCG@",k,":",av_n,end = '\t')
        print("Precision@",k,':',av_p,end = '\t')
        print("Recall@",k,':',av_r,end = '\t')
        print("F1@",k,':',round(av_f,6),end = '\t')
        print()
        
    
    
    