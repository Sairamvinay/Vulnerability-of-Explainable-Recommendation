from __future__ import print_function
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import tensorflow as tf
from tensorflow.python import pywrap_tensorflow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import random
import numpy as np
import pickle
from sklearn.metrics import ndcg_score
from sklearn import metrics
import math
import sys
from tqdm import tqdm
import argparse
from pathlib import Path
from util_a2cf import *
sys.path.append("../../data")
from text_processed_dataset import TextProcessedDataset
from yelp_processed_dataset import YelpProcessedDataset
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['CUDA_VISIBLE_DEVICES'] = '1'
print("TF version:",tf.__version__)

def model_side_evaluation(rec_dict,u_i_exp_dict,sess, predictions, q_final,users,items,feature_vec,masks, rec_dataset,rec_k,args):
    
    test_num = len(list(rec_dict.items()))
    
    ave_pn, ave_ps, ave_fns = evaluate_model_perspective(rec_dict, u_i_exp_dict, sess,predictions, q_final,users,items,feature_vec,masks, rec_dataset.user_feature_matrix, rec_dataset.item_feature_matrix,rec_k,test_num,int(args.dim))
    print('model\'s perspective:')
    print('ave PN: ', round(ave_pn,6), '  ave PS: ', round(ave_ps,6), '  ave F_{NS}: ', round(ave_fns,6))  
    
    
def args_process():    # Parse argument
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", dest="dataset", type=str, default="electronics",help = 'Name of the dataset')
    parser.add_argument("--adv", dest="adv", type=bool, default = False, help="whether to do adversarial or not")
    parser.add_argument("--adv_method", dest="adv_method", type=str, default = 'FGSM', help="whether to do which adversarial method? Random or FGSM")
    parser.add_argument("--dim", type = int, default = 64, help = 'Hidden Embedding dimensions')
    parser.add_argument("--eps", dest="eps", type=float, default=0.5, help="Error proportion for adversarial training")
    parser.add_argument("--seed", dest="seed", type=int, default=999, help="seed for training")
    parser.add_argument("--data_path",dest = 'data_path', type=str, default ='../data/final-obj/Electronics_dataset_obj.pickle',help = "Path to the dataset for loading")
    parser.add_argument("--model_path", dest="model_path", type=str, default="./logs/",help = 'Load the trained model')
    parser.add_argument("--expl_path", dest="expl_path", type=str, default="./logs/",help = 'Load the explanation model')
    parser.add_argument("--save_path",dest='save_path',type=str, default = 'expl-obj/', help = 'Where to save the explanations')
    parser.add_argument("--rec_k", dest="rec_k", type=int, default=20,help = 'Top K items to check validation of explanations')
    parser.add_argument("--exp_k", dest="exp_k", type=int, default=20,help = 'Number of aspect explanations')
    parser.add_argument("--test_num", dest="test_num", type=int, default=-1,help = 'Number of test samples')
    args = parser.parse_args()
    return args
    

def EstPnQ(params):
    '''
    The NCF approach is too slow when estimating the full matrices, hence we replace that with a quicker multiplication form.
    With well trained user/item/attribute embeddings, we find that there are no obvious differences in the actual recommendation performance. 
    '''
    with tf.device("/CPU:0"):
        P = tf.nn.tanh(tf.matmul(params["user_embedding"] + params['user_noise_embedding'], tf.transpose(params["attribute_embedding"] + params["attribute_noise_embedding"])))
        P = (0.5*N - 0.5)*P + 0.5*N + 0.5
        Q = tf.nn.tanh(tf.matmul(params["item_embedding"] + params["item_noise_embedding"], tf.transpose(params["attribute_embedding"] + params["attribute_noise_embedding"])))
        Q = (0.5*N - 0.5)*Q + 0.5*N + 0.5
        
        assign_op_P = tf.assign(params["P"], P)
        assign_op_Q = tf.assign(params["Q"], Q)
    return assign_op_P, assign_op_Q


def cf_prediction(params,user, items, feature_vec,masks,hidden_dim,attribute_dim,epsilon,N = 5):
    user_embedding = tf.reshape(tf.nn.embedding_lookup(params["user_embedding"], user), [-1, hidden_dim]) + tf.reshape(tf.nn.embedding_lookup(params["user_noise_embedding"], user), [-1, hidden_dim]) 
    item_embedding = tf.reshape(tf.nn.embedding_lookup(params["item_embedding"], items), [-1, hidden_dim]) + tf.reshape(tf.nn.embedding_lookup(params["item_noise_embedding"], items), [-1, hidden_dim]) 
    
    p_query = tf.reshape(tf.nn.embedding_lookup(params["P"], user), [-1, attribute_dim])
    
    q_candidate = tf.reshape(tf.nn.embedding_lookup(params["Q"], items), [-1, attribute_dim]) #(batch, attr_size)
    q_candidate = tf.multiply(feature_vec, q_candidate)
    ita = tf.nn.softmax(tf.multiply(p_query, q_candidate)/epsilon, axis = 1) #(batch, attr_size)
    attr_embed_mask = tf.multiply(masks, params["attribute_embedding"] + params["attribute_noise_embedding"])
    v_candidate_tilde_2 = tf.matmul(ita, attr_embed_mask) #(batch, hidden)
    f_p_temp = tf.concat([tf.multiply(user_embedding, item_embedding), v_candidate_tilde_2], 1) #(batch, 2*hidden)
    f_p = tf.matmul(f_p_temp, params["p_projection_weight"] + params["p_noise_projection_weight"]) #(batch, 1)
    f_p = tf.sigmoid(f_p)
    return q_candidate, f_p

'''

network parameters
'''


args = args_process()
print("Args:",args)
with open(args.data_path, 'rb') as handle:
    rec_dataset = pickle.load(handle)

print("DATASET:",args.dataset.upper())
rec_dataset.printDetails()

user_dim = rec_dataset.user_num
item_dim = rec_dataset.item_num
attribute_dim = rec_dataset.feature_num

pos_size = 6 # change based on what test data you use depending on number of positive samples per user
N = 5 # maximum score
hidden_dim = int(args.dim)
beta = 8
epsilon = 8
gamma = 0 # so that we remove item similarity conditions #0.7 # 0<gamma<1
seed = int(args.seed)
adv = bool(args.adv)
adv_method = str(args.adv_method)
eps = float(args.eps)
rec_k = int(args.rec_k)
exp_k = int(args.exp_k)

with open(args.expl_path,'rb') as handle:
    dicts = pickle.load(handle)
    
u_i_expl_dict,rec_dict = dicts[0],dicts[1]


random.seed(seed)
np.random.seed(seed)
tf.set_random_seed(seed)

print("ARGS:",args)

'''
define parameters (weights & biases)
'''
params = {
    #____ embedding weights ____
    "user_embedding": tf.get_variable("user_embedding", shape = [user_dim, hidden_dim], trainable = False, initializer = tf.glorot_uniform_initializer),
    "item_embedding": tf.get_variable("item_embedding", shape = [item_dim, hidden_dim], trainable = False, initializer = tf.glorot_uniform_initializer),
    "attribute_embedding": tf.get_variable("helpful_embedding", shape = [attribute_dim, hidden_dim], trainable = False, initializer = tf.glorot_uniform_initializer),
    #____ noise weights _____
    "user_noise_embedding": tf.get_variable("user_noise_embedding", shape = [user_dim, hidden_dim], trainable = False, initializer = tf.zeros_initializer),
    "item_noise_embedding": tf.get_variable("item_noise_embedding", shape = [item_dim, hidden_dim], trainable = False, initializer = tf.zeros_initializer),
    "attribute_noise_embedding": tf.get_variable("helpful_noise_embedding", shape = [attribute_dim, hidden_dim], trainable = False, initializer = tf.zeros_initializer),
    "p_noise_projection_weight": tf.get_variable("p_noise_projection_weight", shape = [2*hidden_dim, 1], trainable = False, initializer = tf.zeros_initializer),
    
    #____ MF weights ____    
    "s_projection_weight": tf.get_variable("s_projection_weight", shape = [2*hidden_dim, 1], trainable = False, initializer = tf.glorot_uniform_initializer),
    "p_projection_weight": tf.get_variable("p_projection_weight", shape = [2*hidden_dim, 1], trainable = False, initializer = tf.glorot_uniform_initializer)}

with tf.device("/CPU:0"):    
    params["P"] = tf.get_variable("P", trainable = False, initializer = tf.zeros([user_dim, attribute_dim]), caching_device = "/CPU:0")
    params["Q"] = tf.get_variable("Q", trainable = False, initializer = tf.zeros([item_dim, attribute_dim]), caching_device = "/CPU:0")
    

users = tf.placeholder("int32", [None, 1]) #(batch, 1)
items = tf.placeholder("int32", [None, 1]) #(batch, 1)
feature_vec = tf.placeholder("float32",[None, attribute_dim])
masks = tf.placeholder("float32",[None, hidden_dim])
if adv:
    print("ADVERSARIAL MODEL USED: ",args.model_path)

else:
    print("SIMPLE MODEL USED: ",args.model_path)

q_final, predictions_cf = cf_prediction(params, users, items,feature_vec,masks,hidden_dim,attribute_dim, epsilon)

with tf.Session() as sess:
    reader = pywrap_tensorflow.NewCheckpointReader(args.model_path)
    weights = ['user_embedding','item_embedding','helpful_embedding','p_projection_weight', 's_projection_weight','user_noise_embedding','item_noise_embedding','helpful_noise_embedding','p_noise_projection_weight']
    _params = [params['user_embedding'],params['item_embedding'],params['attribute_embedding'],params['p_projection_weight'],params['s_projection_weight'],params['user_noise_embedding'],params['item_noise_embedding'],params['attribute_noise_embedding'],params['p_noise_projection_weight']]
    ops = [] #load all the weights
    for param,weight in zip(_params, weights):
        t = reader.get_tensor(weight)
        ops.append(param.assign(t))


    sess.run(ops)
    new_P, new_Q = EstPnQ(params)
    sess.run([new_P, new_Q])
    
    model_side_evaluation(rec_dict,u_i_expl_dict,sess, predictions_cf, q_final,users,items,feature_vec,masks, rec_dataset,rec_k,args)
    




